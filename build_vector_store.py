import hashlib
import sys
import torch
from langchain.text_splitter import RecursiveCharacterTextSplitter  # å¯¼å…¥æ–‡æ¡£åˆ†å‰²å·¥å…·
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings  # å¯¼å…¥HuggingFaceåµŒå…¥æ¨¡å‹
from langchain_community.vectorstores import FAISS  # å¯¼å…¥FAISSç”¨äºæ„å»ºå‘é‡æ•°æ®åº“
from langchain_community.document_loaders import UnstructuredPDFLoader  # æ–°å¢å¯¼å…¥
from langchain_community.document_loaders import UnstructuredWordDocumentLoader
import os
import json
from pathlib import Path  # å¯¼å…¥Pathï¼Œç”¨äºè·¯å¾„å¤„ç†
from datetime import datetime  # å¯¼å…¥datetimeï¼Œç”¨äºè®°å½•æ—¶é—´æˆ³
from typing import List, Dict, Optional, Set, Tuple  # å¯¼å…¥ç±»å‹æç¤º
import logging  # å¯¼å…¥æ—¥å¿—æ¨¡å—ï¼Œç”¨äºè®°å½•è¿è¡Œæ—¥å¿—
from concurrent.futures import ThreadPoolExecutor, as_completed  # å¯¼å…¥çº¿ç¨‹æ± æ¨¡å—ï¼Œæ”¯æŒå¹¶è¡ŒåŠ è½½PDFæ–‡ä»¶
from tqdm import tqdm  # å¯¼å…¥è¿›åº¦æ¡æ¨¡å—ï¼Œç”¨äºæ˜¾ç¤ºåŠ è½½è¿›åº¦
from config import Config  # å¯¼å…¥é…ç½®ç±»ï¼Œç”¨äºåŠ è½½é…ç½®å‚æ•°
import shutil  # ç”¨äºæ–‡ä»¶æ“ä½œ

from pdf_cor_extractor.pdf_ocr_extractor import PDFProcessor


# é…ç½®æ—¥å¿—æ ¼å¼
# é…ç½®æ—¥å¿—æ ¼å¼ï¼ŒæŒ‡å®šè¾“å‡ºåˆ°stdout
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(stream=sys.stdout)],  # æ˜ç¡®è¾“å‡ºåˆ°stdout
    force=True  # å…³é”®ï¼šå¼ºåˆ¶è¦†ç›–ç°æœ‰é…ç½®
)
logger = logging.getLogger(__name__)


class VectorDBBuilder:
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“æ„å»ºå™¨
        Args:
            config (Config): é…ç½®ç±»ï¼ŒåŒ…å«å¿…è¦çš„é…ç½®
        """
        self.config = config
        
        # è®¾ç½®ç¼“å­˜ç›®å½•è·¯å¾„ï¼ˆä¿ç•™ç”¨äºå­˜å‚¨åˆ†å—åˆ†æç»“æœï¼‰
        self.cache_dir = Path(config.cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®å‘é‡æ•°æ®åº“è·¯å¾„
        self.vector_dir = Path(config.vector_db_path)
        self.vector_backup_dir = self.vector_dir / "backups"
        
        # å°†æºæ–‡ä»¶ç›®å½•å®šä¹‰æ”¾åœ¨åˆå§‹åŒ–æ–¹æ³•ä¸­
        self.subfolders = ['æ ‡å‡†']  # 'æ ‡å‡†æ€§æ–‡ä»¶','æ³•å¾‹', 'è§„èŒƒæ€§æ–‡ä»¶'
        
        # æ£€æŸ¥æ–‡ä»¶åŒ¹é…æ¨¡å¼
        if not hasattr(config, 'files') or not config.files:
            # å¦‚æœconfigä¸­æ²¡æœ‰fileså‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
            self.config.files = ["data/**/*.pdf", "data/**/*.txt", "data/**/*.md", "data/**/*.docx"]
        
        # æ·»åŠ GPUä½¿ç”¨é…ç½®
        self.use_gpu_for_ocr = "cuda" in self.config.device
        
        # å·²å¤„ç†æ–‡ä»¶çŠ¶æ€
        self.failed_files_count = 0
        
        # æ˜¯å¦è¾“å‡ºè¯¦ç»†çš„åˆ†å—å†…å®¹
        self.print_detailed_chunks = getattr(config, 'print_detailed_chunks', False)
        # è¯¦ç»†è¾“å‡ºæ—¶æ¯ä¸ªæ–‡æœ¬å—æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°
        self.max_chunk_preview_length = getattr(config, 'max_chunk_preview_length', 200)
        
        logger.info("åˆå§‹åŒ–å‘é‡æ•°æ®åº“æ„å»ºå™¨...")

    def _is_non_content_page(self, page_content: str, page_num: int) -> bool:
        """
        æ£€æµ‹é¡µé¢æ˜¯å¦ä¸ºéå†…å®¹é¡µé¢ï¼Œå¦‚å°é¢ã€ç›®å½•ã€ç›®æ¬¡ã€å‰è¨€ç­‰ï¼Œè¿™äº›é¡µé¢åœ¨åˆ†å—æ—¶åº”å½“è¢«è¿‡æ»¤æ‰
        
        Args:
            page_content: é¡µé¢æ–‡æœ¬å†…å®¹
            page_num: é¡µé¢ç¼–å·
            
        Returns:
            bool: å¦‚æœæ˜¯éå†…å®¹é¡µé¢è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        # å¦‚æœæ˜¯ç¬¬ä¸€é¡µï¼Œå¾ˆå¯èƒ½æ˜¯å°é¢
        if page_num == 0 or page_num == 1:
            # å°é¢é¡µé€šå¸¸å¾ˆçŸ­ï¼Œæˆ–è€…åªåŒ…å«æ ‡é¢˜ã€ä½œè€…ç­‰ä¿¡æ¯
            if len(page_content.strip()) < 200:
                return True
            
            # å°é¢é¡µé€šå¸¸åŒ…å«è¿™äº›å…³é”®è¯
            cover_keywords = ['å°é¢', 'ç‰ˆæƒ', 'ç‰ˆæƒæ‰€æœ‰', 'å‡ºç‰ˆ', 'ç¼–å†™', 'ç¼–è‘—', 
                             'è‘—ä½œæƒ', 'ä¿ç•™æ‰€æœ‰æƒåˆ©', 'ç‰ˆæƒå£°æ˜', 'ä¿®è®¢ç‰ˆ']
            for keyword in cover_keywords:
                if keyword in page_content:
                    return True
        
        # æ£€æµ‹ç›®å½•ã€ç›®æ¬¡é¡µé¢
        toc_keywords = ['ç›®å½•', 'ç›® å½•', 'ç›®æ¬¡', 'ç›® æ¬¡', 'ç« èŠ‚', 'ç¬¬ä¸€ç« ', 'ç¬¬äºŒç« ', 'ç¬¬ä¸‰ç« ', 
                       'ç¬¬ä¸€éƒ¨åˆ†', 'ç¬¬äºŒéƒ¨åˆ†', 'é™„å½•', 'ç´¢å¼•', 'å‚è€ƒæ–‡çŒ®', 'å†…å®¹', 'ä¸»è¦å†…å®¹']
        
        # å¦‚æœé¡µé¢ä¸­åŒ…å«å¤šä¸ªç›®å½•å…³é”®è¯ï¼Œå¯èƒ½æ˜¯ç›®å½•é¡µ
        keyword_count = sum(1 for keyword in toc_keywords if keyword in page_content)
        if keyword_count >= 2:
            return True
        
        # æ£€æµ‹å‰è¨€é¡µé¢
        preface_keywords = ['å‰è¨€', 'å‰ è¨€', 'åºè¨€', 'åº è¨€', 'å¼•è¨€', 'å¼• è¨€', 'ç»ªè®º', 'æ¦‚è¿°', 
                            'è¯´æ˜', 'ç¼–å†™è¯´æ˜', 'ä½¿ç”¨è¯´æ˜', 'ç¼–åˆ¶è¯´æ˜']
        for keyword in preface_keywords:
            # å¦‚æœå‰è¨€å…³é”®è¯å‡ºç°åœ¨é¡µé¢å¼€å¤´éƒ¨åˆ†ï¼Œå¾ˆå¯èƒ½æ˜¯å‰è¨€é¡µ
            if keyword in page_content[:200] or f"\n{keyword}\n" in page_content:
                logger.info(f"æ£€æµ‹åˆ°å‰è¨€é¡µï¼Œå…³é”®è¯: {keyword}")
                return True
        
        # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å…¸å‹çš„ç›®å½•ç»“æ„ï¼ˆè¡Œé¦–æ˜¯ç« èŠ‚æ ‡é¢˜ï¼Œè¡Œå°¾æ˜¯é¡µç ï¼‰
        lines = page_content.split('\n')
        pattern_count = 0
        for line in lines:
            line = line.strip()
            # åŒ¹é…ç±»ä¼¼ "ç¬¬Xç«  å†…å®¹..........10" çš„æ¨¡å¼
            if line and (line[0] == 'ç¬¬' or line.startswith('é™„å½•')) and line.strip()[-1].isdigit():
                pattern_count += 1
                
        # å¦‚æœæœ‰å¤šè¡Œç¬¦åˆç›®å½•ç‰¹å¾ï¼Œå¯èƒ½æ˜¯ç›®å½•é¡µ
        if pattern_count >= 3:
            return True
        
        return False

    def _load_single_document(self, file_path: Path) -> Optional[List[Document]]:
        """å¤šçº¿ç¨‹åŠ è½½å•ä¸ªæ–‡æ¡£æ–‡ä»¶ï¼ˆæ”¯æŒ PDFã€DOCXã€DOCï¼‰"""
        try:
            file_extension = file_path.suffix.lower()
            docs = []

            if file_extension == ".pdf":
                try:
                    # æ£€æŸ¥PDFé¡µæ•°
                    import fitz
                    with fitz.open(str(file_path)) as doc:
                        page_count = doc.page_count
                        logger.info(f"[æ–‡æ¡£åŠ è½½] PDFæ–‡ä»¶ '{file_path.name}' å…±æœ‰ {page_count} é¡µ")
                        
                    # ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°åˆå§‹åŒ–å¤„ç†å™¨
                    processor = PDFProcessor(
                        file_path=str(file_path), 
                        lang='ch', 
                        use_gpu=self.use_gpu_for_ocr
                    )
                    
                    # æ ¹æ®é¡µæ•°é€‰æ‹©åˆé€‚çš„GPUå‚æ•°é…ç½®
                    if page_count > 30:
                        logger.info(f"[æ–‡æ¡£åŠ è½½] PDFé¡µæ•°è¾ƒå¤š({page_count}é¡µ)ï¼Œåº”ç”¨å¤§æ–‡æ¡£ä¼˜åŒ–é…ç½®")
                        processor.configure_gpu(**self.config.pdf_ocr_large_doc_params)
                    else:
                        # ä½¿ç”¨æ ‡å‡†å‚æ•°é…ç½®
                        processor.configure_gpu(**self.config.pdf_ocr_params)
                    
                    # å¤„ç†PDF
                    docs = processor.process()
                    
                    # è¿‡æ»¤æ‰éå†…å®¹é¡µé¢ï¼ˆå°é¢ã€ç›®å½•ã€å‰è¨€ç­‰ï¼‰
                    if docs:
                        filtered_docs = []
                        filtered_count = 0
                        filtered_types = []
                        
                        for i, doc in enumerate(docs):
                            if not self._is_non_content_page(doc.page_content, i):
                                filtered_docs.append(doc)
                            else:
                                filtered_count += 1
                                # å°è¯•åˆ¤æ–­é¡µé¢ç±»å‹
                                page_type = "éå†…å®¹é¡µé¢"
                                if i == 0 or i == 1:
                                    page_type = "å°é¢"
                                elif "ç›®å½•" in doc.page_content or "ç›®æ¬¡" in doc.page_content:
                                    page_type = "ç›®å½•/ç›®æ¬¡"
                                elif "å‰è¨€" in doc.page_content:
                                    page_type = "å‰è¨€"
                                    
                                filtered_types.append(page_type)
                                logger.info(f"[æ–‡æ¡£åŠ è½½] è¿‡æ»¤æ‰ '{file_path.name}' çš„ç¬¬ {i+1} é¡µï¼ˆç–‘ä¼¼{page_type}ï¼‰")
                                
                        if filtered_count > 0:
                            # æ±‡æ€»è¿‡æ»¤æƒ…å†µ
                            type_summary = {}
                            for t in filtered_types:
                                if t not in type_summary:
                                    type_summary[t] = 0
                                type_summary[t] += 1
                                
                            type_str = ", ".join([f"{k}é¡µ{v}é¡µ" for k, v in type_summary.items()])
                            logger.info(f"[æ–‡æ¡£åŠ è½½] ä» '{file_path.name}' ä¸­è¿‡æ»¤æ‰ {filtered_count} é¡µéå†…å®¹é¡µé¢ï¼ˆ{type_str}ï¼‰")
                            docs = filtered_docs
                    
                    # æ£€æŸ¥å¤„ç†ç»“æœ
                    if docs and len(docs) < page_count * 0.5:
                        logger.warning(f"[æ–‡æ¡£åŠ è½½] è­¦å‘Š: åªè¯†åˆ«å‡º {len(docs)}/{page_count} é¡µï¼Œä½äº50%ï¼Œå¯èƒ½æœ‰é—®é¢˜")
                    elif docs:
                        logger.info(f"[æ–‡æ¡£åŠ è½½] æˆåŠŸè¯†åˆ« {len(docs)}/{page_count} é¡µ")
                    
                    # å¤„ç†åæ¸…ç†å†…å­˜
                    import gc
                    gc.collect()
                    try:
                        import torch
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    except:
                        pass
                        
                except Exception as e:
                    logger.error(f"[æ–‡æ¡£åŠ è½½] å¤„ç†PDFæ–‡ä»¶ '{file_path.name}' å¤±è´¥: {str(e)}")
                    self.failed_files_count += 1
                    return None
                    
            elif file_extension in [".docx", ".doc"]:
                try:
                    # é¦–å…ˆå°è¯•å¯¼å…¥ä¾èµ–æ¨¡å—
                    try:
                        import docx2txt
                    except ImportError:
                        logger.error(f"ç¼ºå°‘å¤„ç†Wordæ–‡æ¡£æ‰€éœ€çš„ä¾èµ–åŒ…ï¼Œè¯·è¿è¡Œ: pip install docx2txt")
                        # è®°å½•é”™è¯¯ä½†ç»§ç»­æ‰§è¡Œï¼Œä»¥ä¾¿å¤„ç†å…¶ä»–æ–‡ä»¶ç±»å‹
                        self.failed_files_count += 1
                        return None
                        
                    from langchain_community.document_loaders import Docx2txtLoader
                    loader = Docx2txtLoader(str(file_path))
                    docs = loader.load()
                    
                    # å°è¯•è¿‡æ»¤Wordæ–‡æ¡£çš„éå†…å®¹é¡µé¢
                    if docs and len(docs) > 1:  # å¦‚æœWordæ–‡æ¡£è¢«åˆ†æˆäº†å¤šä¸ªé¡µé¢
                        filtered_docs = []
                        filtered_count = 0
                        filtered_types = []
                        
                        for i, doc in enumerate(docs):
                            if not self._is_non_content_page(doc.page_content, i):
                                filtered_docs.append(doc)
                            else:
                                filtered_count += 1
                                # å°è¯•åˆ¤æ–­é¡µé¢ç±»å‹
                                page_type = "éå†…å®¹é¡µé¢"
                                if i == 0 or i == 1:
                                    page_type = "å°é¢"
                                elif "ç›®å½•" in doc.page_content or "ç›®æ¬¡" in doc.page_content:
                                    page_type = "ç›®å½•/ç›®æ¬¡"
                                elif "å‰è¨€" in doc.page_content:
                                    page_type = "å‰è¨€"
                                    
                                filtered_types.append(page_type)
                                logger.info(f"[æ–‡æ¡£åŠ è½½] è¿‡æ»¤æ‰ '{file_path.name}' çš„ç¬¬ {i+1} éƒ¨åˆ†ï¼ˆç–‘ä¼¼{page_type}ï¼‰")
                                
                        if filtered_count > 0:
                            # æ±‡æ€»è¿‡æ»¤æƒ…å†µ
                            type_summary = {}
                            for t in filtered_types:
                                if t not in type_summary:
                                    type_summary[t] = 0
                                type_summary[t] += 1
                                
                            type_str = ", ".join([f"{k}{v}é¡µ" for k, v in type_summary.items()])
                            logger.info(f"[æ–‡æ¡£åŠ è½½] ä» '{file_path.name}' ä¸­è¿‡æ»¤æ‰ {filtered_count} éƒ¨åˆ†éå†…å®¹é¡µé¢ï¼ˆ{type_str}ï¼‰")
                            docs = filtered_docs
                except Exception as e:
                    logger.error(f"[æ–‡æ¡£åŠ è½½] å¤„ç†DOCXæ–‡ä»¶ '{file_path.name}' å¤±è´¥: {str(e)}")
                    
                    # å°è¯•ä½¿ç”¨æ›¿ä»£æ–¹æ³•
                    try:
                        logger.info(f"[æ–‡æ¡£åŠ è½½] å°è¯•ä½¿ç”¨æ›¿ä»£æ–¹æ³•åŠ è½½Wordæ–‡æ¡£...")
                        from langchain_community.document_loaders import UnstructuredWordDocumentLoader
                        loader = UnstructuredWordDocumentLoader(str(file_path))
                        docs = loader.load()
                        logger.info(f"[æ–‡æ¡£åŠ è½½] æˆåŠŸä½¿ç”¨æ›¿ä»£æ–¹æ³•åŠ è½½Wordæ–‡æ¡£: {file_path.name}")
                        
                        # ä¹Ÿå°è¯•è¿‡æ»¤éå†…å®¹é¡µé¢
                        if docs and len(docs) > 1:
                            filtered_docs = []
                            filtered_count = 0
                            filtered_types = []
                            
                            for i, doc in enumerate(docs):
                                if not self._is_non_content_page(doc.page_content, i):
                                    filtered_docs.append(doc)
                                else:
                                    filtered_count += 1
                                    # å°è¯•åˆ¤æ–­é¡µé¢ç±»å‹
                                    page_type = "éå†…å®¹é¡µé¢"
                                    if i == 0 or i == 1:
                                        page_type = "å°é¢"
                                    elif "ç›®å½•" in doc.page_content or "ç›®æ¬¡" in doc.page_content:
                                        page_type = "ç›®å½•/ç›®æ¬¡"
                                    elif "å‰è¨€" in doc.page_content:
                                        page_type = "å‰è¨€"
                                        
                                    filtered_types.append(page_type)
                                    logger.info(f"[æ–‡æ¡£åŠ è½½] è¿‡æ»¤æ‰ '{file_path.name}' çš„ç¬¬ {i+1} éƒ¨åˆ†ï¼ˆç–‘ä¼¼{page_type}ï¼‰")
                                    
                            if filtered_count > 0:
                                # æ±‡æ€»è¿‡æ»¤æƒ…å†µ
                                type_summary = {}
                                for t in filtered_types:
                                    if t not in type_summary:
                                        type_summary[t] = 0
                                    type_summary[t] += 1
                                    
                                type_str = ", ".join([f"{k}{v}é¡µ" for k, v in type_summary.items()])
                                logger.info(f"[æ–‡æ¡£åŠ è½½] ä» '{file_path.name}' ä¸­è¿‡æ»¤æ‰ {filtered_count} éƒ¨åˆ†éå†…å®¹é¡µé¢ï¼ˆ{type_str}ï¼‰")
                                docs = filtered_docs
                    except Exception as e2:
                        logger.error(f"[æ–‡æ¡£åŠ è½½] æ›¿ä»£æ–¹æ³•ä¹Ÿå¤±è´¥: {str(e2)}")
                        self.failed_files_count += 1
                        return None
            else:
                logger.warning(f"[æ–‡æ¡£åŠ è½½] ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.name}")
                return None

            if docs:
                # ç»Ÿä¸€æ·»åŠ å…ƒæ•°æ®
                for doc in docs:
                    doc.metadata["source"] = str(file_path)
                    doc.metadata["file_name"] = file_path.name
                return docs
            return None

        except Exception as e:
            logger.error(f"[æ–‡æ¡£åŠ è½½] åŠ è½½ {file_path} å¤±è´¥: {str(e)}")
            self.failed_files_count += 1
            return None

    def load_documents(self) -> List:
        """åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
        logger.info("âŒ› å¼€å§‹åŠ è½½æ–‡æ¡£...")

        # è·å–æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
        document_files = []
        for subfolder in self.subfolders:
            folder_path = self.config.data_dir / subfolder
            if folder_path.exists() and folder_path.is_dir():
                document_files.extend([f for f in folder_path.rglob("*") 
                                    if f.suffix.lower() in ['.pdf', '.docx', '.doc']])
            else:
                logger.warning(f"å­æ–‡ä»¶å¤¹ {subfolder} ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {folder_path}")
                
        # è¿‡æ»¤å¹¶æ’åºæ–‡ä»¶ï¼ˆå…ˆå¤„ç†è¾ƒå°çš„æ–‡ä»¶ï¼Œé¿å…å¤§æ–‡ä»¶å ç”¨æ˜¾å­˜ï¼‰
        document_files = sorted(document_files, key=lambda x: x.stat().st_size)
        logger.info(f"å‘ç° {len(document_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
        
        results = []
        # é™åˆ¶çº¿ç¨‹æ± å¤§å°ä»¥é¿å…èµ„æºäº‰ç”¨
        with ThreadPoolExecutor(max_workers=1) as executor:
            futures = [executor.submit(self._load_single_document, file) for file in document_files]
            with tqdm(total=len(futures), desc="åŠ è½½æ–‡æ¡£", unit="files") as pbar:
                for future in as_completed(futures):
                    res = future.result()
                    if res:
                        results.extend(res)
                        pbar.update(1)
                        pbar.set_postfix_str(f"å·²åŠ è½½ {len(res)} é¡µ")
                    else:
                        pbar.update(1)
        
        # åœ¨å¤„ç†å®Œæˆåæ¸…ç†GPUç¼“å­˜
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except:
            pass
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(results)} é¡µæ–‡æ¡£")
        logger.info(f"âŒ æœªæˆåŠŸåŠ è½½ {self.failed_files_count} ä¸ªæ–‡ä»¶")
        
        return results

    def process_files(self) -> List:
        """ä¼˜åŒ–çš„æ–‡ä»¶å¤„ç†æµç¨‹ï¼Œä½¿ç”¨ç« èŠ‚åˆ†å—æ–¹æ³•"""
        logger.info("å¼€å§‹æ–‡ä»¶å¤„ç†æµç¨‹")
        
        # åŠ è½½æ‰€æœ‰æ–‡æ¡£
        all_docs = self.load_documents()

        if not all_docs:
            logger.warning("æ²¡æœ‰å¯å¤„ç†çš„æ–‡ä»¶å†…å®¹")
            return []

        # é¦–å…ˆæŒ‰æ–‡ä»¶åˆå¹¶é¡µé¢å†…å®¹ï¼Œé¿å…è·¨é¡µåˆ†å—æ–­è£‚
        logger.info("åˆå¹¶æ–‡ä»¶é¡µé¢å†…å®¹ï¼Œå‡†å¤‡è¿›è¡Œæ•´ä½“åˆ†å—...")
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„æ•´ç†æ–‡æ¡£
        file_docs = {}
        for doc in all_docs:
            source = doc.metadata.get("source", "")
            if source not in file_docs:
                file_docs[source] = []
            file_docs[source].append(doc)
        
        # å¯¹æ¯ä¸ªæ–‡ä»¶çš„é¡µé¢è¿›è¡Œæ’åºå’Œåˆå¹¶
        whole_docs = []
        for source, docs in file_docs.items():
            # æŒ‰é¡µç æ’åº
            sorted_docs = sorted(docs, key=lambda x: x.metadata.get("page", 0))
            
            # åˆå¹¶æ–‡ä»¶æ‰€æœ‰é¡µé¢çš„å†…å®¹
            full_content = "\n\n".join([doc.page_content for doc in sorted_docs])
            
            # åˆ›å»ºå®Œæ•´æ–‡æ¡£å¯¹è±¡
            file_doc = Document(
                page_content=full_content,
                metadata={
                    "source": source,
                    "file_name": sorted_docs[0].metadata.get("file_name", ""),
                    "page_count": len(sorted_docs),
                    "is_merged_doc": True  # æ ‡è®°ä¸ºåˆå¹¶åçš„å®Œæ•´æ–‡æ¡£
                }
            )
            whole_docs.append(file_doc)
            
        logger.info(f"å·²å°† {len(all_docs)} é¡µå†…å®¹åˆå¹¶ä¸º {len(whole_docs)} ä¸ªå®Œæ•´æ–‡æ¡£")
        
        # ä½¿ç”¨æŒ‰ç« èŠ‚åˆ†å—æ–¹æ³•
        logger.info("ä½¿ç”¨ç« èŠ‚åˆ†å—æ–¹æ³•ï¼ŒæŒ‰ç…§æ ‡å‡†æ–‡æ¡£ç»“æ„è¿›è¡Œåˆ†å—...")
        chunks = []
        
        with tqdm(total=len(whole_docs), desc="å¤„ç†æ–‡æ¡£ç« èŠ‚åˆ†å—") as pbar:
            for doc in whole_docs:
                metadata = doc.metadata.copy()
                # ç§»é™¤åˆ†å—åä¸å†é€‚ç”¨çš„å…ƒæ•°æ®
                if "is_merged_doc" in metadata:
                    del metadata["is_merged_doc"]
                
                # æŒ‰ç« èŠ‚åˆ†å—
                sections = self._split_by_section(doc.page_content)
                
                # å¦‚æœæ‰¾åˆ°ç« èŠ‚ï¼Œåˆ™ä½¿ç”¨ç« èŠ‚åˆ†å—
                if sections:
                    logger.info(f"æ‰¾åˆ° {len(sections)} ä¸ªç« èŠ‚ï¼Œä½¿ç”¨ç« èŠ‚ç»“æ„è¿›è¡Œåˆ†å—")
                    for i, (title, content, section_meta) in enumerate(sections):
                        if not content.strip():  # è·³è¿‡ç©ºç« èŠ‚
                            continue
                            
                        # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
                        content_hash = hashlib.md5(content.encode()).hexdigest()
                        
                        # åˆå¹¶å…ƒæ•°æ®
                        enhanced_metadata = metadata.copy()
                        enhanced_metadata.update(section_meta)  # æ·»åŠ ç« èŠ‚å…ƒæ•°æ®
                        enhanced_metadata["content_hash"] = content_hash
                        enhanced_metadata["chunk_index"] = i
                        enhanced_metadata["total_chunks"] = len(sections)
                        enhanced_metadata["chunk_type"] = "section"
                        
                        # æ·»åŠ å—ä½ç½®æ ‡è®°
                        if i == 0:
                            enhanced_metadata["position"] = "document_start"
                        elif i == len(sections) - 1:
                            enhanced_metadata["position"] = "document_end"
                        else:
                            enhanced_metadata["position"] = "document_middle"
                        
                        chunks.append(Document(
                            page_content=content,
                            metadata=enhanced_metadata
                        ))
                else:
                    # å¦‚æœæœªæ‰¾åˆ°ç« èŠ‚ç»“æ„ï¼Œåˆ™å›é€€åˆ°å¸¸è§„åˆ†å—
                    logger.warning(f"æœªæ£€æµ‹åˆ°ç« èŠ‚ç»“æ„ï¼Œå›é€€åˆ°å¸¸è§„åˆ†å—æ–¹æ³•")
                    
                    # ä¼˜åŒ–åçš„æ–‡æœ¬åˆ†å‰²é…ç½®
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=self.config.chunk_size,
                        chunk_overlap=self.config.chunk_overlap,
                        separators=[
                            "\n\n", "\n", "ã€‚", "ï¼›", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""
                        ],
                        length_function=len,
                        add_start_index=True,
                        is_separator_regex=False
                    )
                    
                    # å¯¹å®Œæ•´æ–‡æ¡£è¿›è¡Œåˆ†å—
                    split_texts = text_splitter.split_text(doc.page_content)
                    
                    # å¤„ç†æ¯ä¸ªæ–‡æœ¬å—
                    for i, text in enumerate(split_texts):
                        # åº”ç”¨æ™ºèƒ½è¾¹ç•Œå¤„ç†ï¼Œç¡®ä¿å®Œæ•´å¥å­
                        text = self._ensure_complete_sentences(text)
                        if not text.strip():  # è·³è¿‡ç©ºæ–‡æœ¬å—
                            continue
                            
                        # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
                        content_hash = hashlib.md5(text.encode()).hexdigest()
                        enhanced_metadata = metadata.copy()
                        enhanced_metadata["content_hash"] = content_hash
                        enhanced_metadata["chunk_index"] = i
                        enhanced_metadata["total_chunks"] = len(split_texts)
                        enhanced_metadata["chunk_type"] = "fixed_size"
                        
                        # æ·»åŠ å—ä½ç½®æ ‡è®°
                        if i == 0:
                            enhanced_metadata["position"] = "document_start"
                        elif i == len(split_texts) - 1:
                            enhanced_metadata["position"] = "document_end"
                        else:
                            enhanced_metadata["position"] = "document_middle"

                        chunks.append(Document(
                            page_content=text,
                            metadata=enhanced_metadata
                        ))
                    
                pbar.update(1)
        
        # åº”ç”¨åå¤„ç†ï¼Œç¡®ä¿æ–‡æœ¬å—çš„å®Œæ•´æ€§å’Œè¿è´¯æ€§
        chunks = self._post_process_chunks(chunks)
        
        logger.info(f"ç”Ÿæˆ {len(chunks)} ä¸ªè¯­ä¹‰è¿è´¯çš„æ–‡æœ¬å—")
        
        # æ‰“å°åˆ†å—ç»“æœæ¦‚è§ˆ
        self._print_chunks_summary(chunks)
        
        # ä¿å­˜åˆ†å—åˆ°æ–‡ä»¶ï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥çœ‹
        self.save_chunks_to_file(chunks)

        return chunks

    def _split_by_section(self, text: str) -> List[Tuple[str, str, Dict]]:
        """
        æ ¹æ®ç« èŠ‚æ ‡é¢˜ï¼ˆå¦‚1ã€1.1ã€1.1.1æ ¼å¼ï¼‰å°†æ–‡æœ¬åˆ†å‰²æˆæ®µè½
        
        Args:
            text: å®Œæ•´çš„æ–‡æ¡£æ–‡æœ¬
            
        Returns:
            List[Tuple[str, str, Dict]]: è¿”å›ç« èŠ‚æ ‡é¢˜ã€ç« èŠ‚å†…å®¹å’Œå…ƒæ•°æ®çš„å…ƒç»„åˆ—è¡¨
        """
        logger.info("å¼€å§‹æŒ‰ç« èŠ‚ç»“æ„è¿›è¡Œæ–‡æ¡£åˆ†å—...")
        
        # å®šä¹‰ç« èŠ‚æ ‡é¢˜æ¨¡å¼çš„æ­£åˆ™è¡¨è¾¾å¼
        # åŒ¹é…å½¢å¦‚"1."ã€"1.1"ã€"1.1.1"ç­‰çš„ç« èŠ‚æ ‡è®°ï¼Œåé¢è·Ÿç€ç©ºæ ¼å’Œæ ‡é¢˜æ–‡æœ¬
        section_patterns = [
            # åŒ¹é…ç¬¬ä¸€çº§æ ‡é¢˜ï¼šæ•°å­—+ç‚¹ï¼Œå¦‚"1. æ€»åˆ™"
            r'^\s*(\d+)\.?\s+([^\n]+)$',
            # åŒ¹é…ç¬¬äºŒçº§æ ‡é¢˜ï¼šæ•°å­—.æ•°å­—ï¼Œå¦‚"1.1 é€‚ç”¨èŒƒå›´"
            r'^\s*(\d+\.\d+)\.?\s+([^\n]+)$',
            # åŒ¹é…ç¬¬ä¸‰çº§æ ‡é¢˜ï¼šæ•°å­—.æ•°å­—.æ•°å­—ï¼Œå¦‚"1.1.1 åŸºæœ¬è¦æ±‚"
            r'^\s*(\d+\.\d+\.\d+)\.?\s+([^\n]+)$',
            # åŒ¹é…å¯èƒ½çš„ç¬¬å››çº§æ ‡é¢˜ï¼šæ•°å­—.æ•°å­—.æ•°å­—.æ•°å­—
            r'^\s*(\d+\.\d+\.\d+\.\d+)\.?\s+([^\n]+)$',
            # åŒ¹é…ä¸­æ–‡åºå·æ ‡é¢˜ï¼šä¸€ã€äºŒã€ä¸‰...
            r'^\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ã€.ï¼]\s+([^\n]+)$',
            # åŒ¹é…æ‹¬å·åºå·æ ‡é¢˜ï¼šï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰...
            r'^\s*[ï¼ˆ(]([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[)ï¼‰]\s+([^\n]+)$',
            # åŒ¹é…é™„å½•æ ¼å¼ï¼šé™„å½•Aã€é™„å½•Bç­‰
            r'^\s*(é™„å½•\s*[A-Za-z])[.ï¼ã€]?\s*([^\n]+)?$'
        ]
        
        # å°†æ–‡æœ¬æŒ‰è¡Œåˆ†å‰²
        lines = text.split('\n')
        
        # åˆå§‹åŒ–ç»“æœåˆ—è¡¨å’Œå½“å‰ç« èŠ‚ä¿¡æ¯
        sections = []
        current_section_num = ""
        current_section_title = ""
        current_section_content = []
        current_section_level = 0
        
        # ç”¨äºæ ‡è®°æ˜¯å¦å·²ç»æ‰¾åˆ°ç¬¬ä¸€ä¸ªç« èŠ‚æ ‡é¢˜
        found_first_section = False
        
        # é€è¡Œå¤„ç†æ–‡æœ¬
        for line_num, line in enumerate(lines):
            line = line.strip()
            if not line:
                # ç©ºè¡Œï¼Œæ·»åŠ åˆ°å½“å‰ç« èŠ‚å†…å®¹
                if current_section_content:
                    current_section_content.append("")
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç« èŠ‚æ ‡é¢˜
            is_section_header = False
            section_level = 0
            section_num = ""
            section_title = ""
            
            for i, pattern in enumerate(section_patterns):
                import re
                match = re.match(pattern, line)
                if match:
                    section_level = i + 1  # ç« èŠ‚çº§åˆ«
                    section_num = match.group(1)  # ç« èŠ‚ç¼–å·
                    
                    # å¤„ç†æ ‡é¢˜ï¼Œå¦‚æœåŒ¹é…ç»„2å­˜åœ¨
                    if len(match.groups()) > 1 and match.group(2):
                        section_title = match.group(2).strip()
                    else:
                        # å¦‚æœæ²¡æœ‰æ ‡é¢˜æ–‡æœ¬ï¼Œä½¿ç”¨ç« èŠ‚ç¼–å·ä½œä¸ºæ ‡é¢˜
                        section_title = section_num
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ªçœŸæ­£çš„ç« èŠ‚æ ‡é¢˜
                    # é€šå¸¸æ–‡æ¡£å¼€å§‹ä¼šæœ‰"1. æ€»åˆ™"æˆ–"1. èŒƒå›´"è¿™æ ·çš„æ ‡é¢˜
                    if not found_first_section and section_level == 1:
                        found_first_section = True
                    
                    is_section_header = True
                    break
            
            # å¦‚æœå½“å‰è¡Œæ˜¯ç« èŠ‚æ ‡é¢˜ï¼Œä¸”å·²æ‰¾åˆ°ç¬¬ä¸€ä¸ªç« èŠ‚æ ‡é¢˜
            if is_section_header and (found_first_section or "é™„å½•" in section_num):
                # ä¿å­˜å…ˆå‰ç« èŠ‚çš„å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
                if current_section_num and current_section_content:
                    # åˆå¹¶å½“å‰ç« èŠ‚çš„å†…å®¹
                    content = "\n".join(current_section_content)
                    # åˆ›å»ºå…ƒæ•°æ®
                    metadata = {
                        "section_num": current_section_num,
                        "section_title": current_section_title,
                        "section_level": current_section_level
                    }
                    # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                    sections.append((current_section_num + " " + current_section_title, content, metadata))
                
                # å¼€å§‹æ–°ç« èŠ‚
                current_section_num = section_num
                current_section_title = section_title
                current_section_level = section_level
                current_section_content = [line]  # åŒ…å«ç« èŠ‚æ ‡é¢˜è¡Œ
            else:
                # éç« èŠ‚æ ‡é¢˜è¡Œï¼Œæ·»åŠ åˆ°å½“å‰ç« èŠ‚å†…å®¹
                if current_section_content or not found_first_section:
                    current_section_content.append(line)
        
        # å¤„ç†æœ€åä¸€ä¸ªç« èŠ‚
        if current_section_num and current_section_content:
            content = "\n".join(current_section_content)
            metadata = {
                "section_num": current_section_num,
                "section_title": current_section_title,
                "section_level": current_section_level
            }
            sections.append((current_section_num + " " + current_section_title, content, metadata))
        
        # å¤„ç†å¯èƒ½å­˜åœ¨çš„å‰å¯¼å†…å®¹ï¼ˆåœ¨ç¬¬ä¸€ä¸ªç« èŠ‚æ ‡é¢˜ä¹‹å‰ï¼‰
        if not found_first_section and current_section_content:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç« èŠ‚æ ‡é¢˜ï¼Œä½†ä»æœ‰å†…å®¹
            # åˆ›å»ºä¸€ä¸ª"å¼•è¨€"ç« èŠ‚
            content = "\n".join(current_section_content)
            metadata = {
                "section_num": "0",
                "section_title": "å¼•è¨€",
                "section_level": 0
            }
            sections.append(("å¼•è¨€", content, metadata))
        
        # æ‰“å°ç« èŠ‚åˆ†å—ç»“æœ
        logger.info(f"æŒ‰ç« èŠ‚ç»“æ„åˆ†å—å®Œæˆï¼Œå…±æ‰¾åˆ° {len(sections)} ä¸ªç« èŠ‚")
        for i, (title, _, meta) in enumerate(sections[:min(5, len(sections))]):
            logger.info(f"  â€¢ ç« èŠ‚ {i+1}: {title} (çº§åˆ«: {meta['section_level']})")
        
        if len(sections) > 5:
            logger.info(f"  ... ä»¥åŠ {len(sections)-5} ä¸ªå…¶ä»–ç« èŠ‚")
        
        return sections

    def _ensure_complete_sentences(self, text: str) -> str:
        """ç¡®ä¿æ–‡æœ¬å—ä»¥å®Œæ•´å¥å­å¼€å§‹å’Œç»“æŸ
        
        Args:
            text: åŸå§‹æ–‡æœ¬å—
            
        Returns:
            å¤„ç†åçš„æ–‡æœ¬å—ï¼Œç¡®ä¿ä»¥å®Œæ•´å¥å­å¼€å§‹å’Œç»“æŸ
        """
        if not text or len(text) < 10:  # æ–‡æœ¬è¿‡çŸ­åˆ™ç›´æ¥è¿”å›
            return text
            
        # ä¸­æ–‡å¥å­ç»“æŸæ ‡è®°
        sentence_end_marks = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', '\n']
        # å¥å­å¼€å§‹çš„å¯èƒ½æ ‡è®°ï¼ˆä¸­æ–‡æ®µè½å¼€å¤´ã€ç« èŠ‚æ ‡é¢˜ç­‰ï¼‰
        sentence_start_patterns = ['\n', 'ç¬¬.{1,3}ç« ', 'ç¬¬.{1,3}èŠ‚']
        
        # å¤„ç†æ–‡æœ¬å—å¼€å¤´
        text_stripped = text.lstrip()
        # å¦‚æœä¸æ˜¯ä»¥å¥æœ«æ ‡ç‚¹å¼€å¤´ï¼Œä¹Ÿä¸æ˜¯ä»¥å¤§å†™å­—æ¯æˆ–æ•°å­—å¼€å¤´ï¼ˆå¯èƒ½æ˜¯æ–°æ®µè½ï¼‰ï¼Œåˆ™å¯èƒ½æ˜¯ä¸å®Œæ•´å¥å­
        is_incomplete_start = True
        
        # æ£€æŸ¥æ˜¯å¦ä»¥å®Œæ•´å¥å­æˆ–æ®µè½å¼€å§‹çš„æ ‡è®°
        for pattern in sentence_start_patterns:
            if text.startswith(pattern) or text_stripped[0].isupper() or text_stripped[0].isdigit():
                is_incomplete_start = False
                break
        
        if is_incomplete_start:
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®Œæ•´å¥å­çš„å¼€å§‹
            for mark in sentence_end_marks:
                pos = text.find(mark)
                if pos > 0:
                    # æ‰¾åˆ°å¥æœ«æ ‡è®°åçš„å†…å®¹ä½œä¸ºèµ·ç‚¹
                    try:
                        # ç¡®ä¿å¥æœ«æ ‡è®°åè¿˜æœ‰å†…å®¹
                        if pos + 1 < len(text):
                            text = text[pos+1:].lstrip()
                            break
                    except:
                        # å‡ºé”™åˆ™ä¿æŒåŸæ ·
                        pass
        
        # å¤„ç†æ–‡æœ¬å—ç»“å°¾
        is_incomplete_end = True
        # æ£€æŸ¥æ˜¯å¦ä»¥å®Œæ•´å¥å­ç»“æŸ
        for mark in sentence_end_marks:
            if text.endswith(mark):
                is_incomplete_end = False
                break
        
        if is_incomplete_end:
            # æ‰¾æœ€åä¸€ä¸ªå®Œæ•´å¥å­çš„ç»“æŸä½ç½®
            last_pos = -1
            for mark in sentence_end_marks:
                pos = text.rfind(mark)
                if pos > last_pos:
                    last_pos = pos
                    
            if last_pos > 0:
                # æˆªå–åˆ°æœ€åä¸€ä¸ªå®Œæ•´å¥å­ç»“æŸ
                text = text[:last_pos+1]
        
        return text.strip()
    
    def _post_process_chunks(self, chunks: List[Document]) -> List[Document]:
        """å¯¹åˆ†å—åçš„æ–‡æœ¬è¿›è¡Œåå¤„ç†ï¼Œä¼˜åŒ–å—çš„è´¨é‡
        
        Args:
            chunks: åŸå§‹åˆ†å—åˆ—è¡¨
            
        Returns:
            å¤„ç†åçš„åˆ†å—åˆ—è¡¨
        """
        if not chunks:
            return []
            
        logger.info("å¯¹æ–‡æœ¬å—è¿›è¡Œåå¤„ç†ä¼˜åŒ–...")
        processed_chunks = []
        
        # æŒ‰æ–‡æ¡£æºåˆ†ç»„å¤„ç†
        doc_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "")
            if source not in doc_chunks:
                doc_chunks[source] = []
            doc_chunks[source].append(chunk)
        
        total_merged = 0
        
        # å¤„ç†æ¯ä¸ªæ–‡æ¡£çš„å—
        for source, source_chunks in doc_chunks.items():
            # æŒ‰å—ç´¢å¼•æ’åº
            sorted_chunks = sorted(source_chunks, 
                                   key=lambda x: x.metadata.get("chunk_index", 0))
            
            # æ£€æŸ¥å’Œå¤„ç†ç›¸é‚»å—
            for i, chunk in enumerate(sorted_chunks):
                # ç¡®ä¿å®Œæ•´å¥å­
                chunk.page_content = self._ensure_complete_sentences(chunk.page_content)
                
                # è·³è¿‡ç©ºå—
                if not chunk.page_content.strip():
                    continue
                
                # è¿‡æ»¤æ‰è¿‡çŸ­çš„å—ï¼ˆä¾‹å¦‚åªæœ‰å‡ ä¸ªå­—çš„å—ï¼‰
                if len(chunk.page_content) < 50:  # è®¾ç½®æœ€å°å—é•¿åº¦é˜ˆå€¼
                    continue
                
                # æ£€æŸ¥ä¸å‰ä¸€ä¸ªå—çš„é‡å åº¦
                if i > 0 and processed_chunks:
                    prev_chunk = processed_chunks[-1]
                    if prev_chunk.metadata.get("source") == source:
                        # è®¡ç®—é‡å åº¦
                        overlap_ratio = self._calculate_overlap_ratio(
                            prev_chunk.page_content, chunk.page_content)
                        
                        # å¦‚æœé‡å åº¦è¿‡é«˜ï¼ˆè¶…è¿‡70%ï¼‰ï¼Œè€ƒè™‘åˆå¹¶æˆ–è·³è¿‡
                        if overlap_ratio > 0.7:
                            # å¦‚æœå½“å‰å—æ¯”å‰ä¸€ä¸ªå—çŸ­ï¼Œè·³è¿‡å½“å‰å—
                            if len(chunk.page_content) <= len(prev_chunk.page_content):
                                continue
                            # å¦åˆ™ç”¨å½“å‰å—æ›¿æ¢å‰ä¸€ä¸ªå—
                            else:
                                processed_chunks[-1] = chunk
                                continue
                
                processed_chunks.append(chunk)
        
        logger.info(f"åå¤„ç†å®Œæˆï¼Œä¼˜åŒ–åçš„å—æ•°: {len(processed_chunks)}")
        return processed_chunks
        
    def _calculate_overlap_ratio(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„é‡å æ¯”ä¾‹
        
        Args:
            text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
            text2: ç¬¬äºŒä¸ªæ–‡æœ¬
            
        Returns:
            é‡å æ¯”ä¾‹ï¼ˆ0-1ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼‰
        """
        # ä½¿ç”¨è¾ƒçŸ­æ–‡æœ¬çš„é•¿åº¦ä½œä¸ºåˆ†æ¯
        min_len = min(len(text1), len(text2))
        if min_len == 0:
            return 0.0
            
        # å¯»æ‰¾æœ€é•¿çš„å…±åŒå­ä¸²
        for i in range(min_len, 0, -1):
            if text1.endswith(text2[:i]):
                return i / min_len
            if text2.endswith(text1[:i]):
                return i / min_len
                
        return 0.0

    def _print_chunks_summary(self, chunks: List[Document]):
        """æ‰“å°æ–‡æœ¬åˆ†å—ç»“æœæ¦‚è§ˆ"""
        if not chunks:
            logger.info("æ²¡æœ‰æ–‡æœ¬å—å¯ä¾›æ˜¾ç¤º")
            return
            
        # ç»Ÿè®¡ä¿¡æ¯
        total_chunks = len(chunks)
        avg_chunk_length = sum(len(chunk.page_content) for chunk in chunks) / total_chunks
        files_count = len(set(chunk.metadata.get("source", "") for chunk in chunks))
        
        logger.info("\n" + "="*50)
        logger.info("ğŸ“Š æ–‡æœ¬åˆ†å—å¤„ç†æ¦‚è§ˆ")
        logger.info("="*50)
        logger.info(f"ğŸ“„ æ€»å—æ•°: {total_chunks}")
        logger.info(f"ğŸ“Š å¹³å‡å—é•¿åº¦: {avg_chunk_length:.1f} å­—ç¬¦")
        logger.info(f"ğŸ“‚ æ¶‰åŠæ–‡ä»¶æ•°: {files_count}")
        
        # æ–‡ä»¶çº§ç»Ÿè®¡
        file_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
            if source not in file_chunks:
                file_chunks[source] = []
            file_chunks[source].append(chunk)
        
        logger.info("\nğŸ“‚ æ–‡ä»¶çº§åˆ†å—ç»Ÿè®¡:")
        for file_path, file_chunks_list in sorted(file_chunks.items(), key=lambda x: len(x[1]), reverse=True):
            file_name = Path(file_path).name if isinstance(file_path, str) else "æœªçŸ¥æ–‡ä»¶"
            logger.info(f"  â€¢ {file_name}: {len(file_chunks_list)} å—")
        
        
        # è¾“å‡ºè¯¦ç»†åˆ†å—å†…å®¹ (å¦‚æœå¼€å¯)
        if self.print_detailed_chunks:
            self._print_detailed_chunks(chunks)
            
        logger.info("="*50)

    def _print_detailed_chunks(self, chunks: List[Document]):
        """è¾“å‡ºè¯¦ç»†çš„åˆ†å—å†…å®¹"""
        logger.info("\n" + "="*50)
        logger.info("ğŸ“‘ è¯¦ç»†æ–‡æœ¬å—å†…å®¹")
        logger.info("="*50)
        
        # å°†åˆ†å—æŒ‰æ–‡ä»¶åˆ†ç»„
        file_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
            if source not in file_chunks:
                file_chunks[source] = []
            file_chunks[source].append(chunk)
        
        # ä¸ºäº†æ›´æœ‰ç»„ç»‡åœ°è¾“å‡ºï¼Œå…ˆæŒ‰æ–‡ä»¶è¾“å‡º
        for file_path, file_chunks_list in sorted(file_chunks.items(), key=lambda x: len(x[1]), reverse=True):
            file_name = Path(file_path).name if isinstance(file_path, str) else "æœªçŸ¥æ–‡ä»¶"
            logger.info(f"\nğŸ“„ æ–‡ä»¶: {file_name} (å…±{len(file_chunks_list)}å—)")
            
            # è¾“å‡ºè¯¥æ–‡ä»¶çš„å‰3ä¸ªå—
            for i, chunk in enumerate(file_chunks_list[:3]):
                page_num = chunk.metadata.get("page", "æœªçŸ¥é¡µç ")
                chunk_size = len(chunk.page_content)
                
                # è·å–é¢„è§ˆå†…å®¹
                content_preview = chunk.page_content
                if len(content_preview) > self.max_chunk_preview_length:
                    content_preview = content_preview[:self.max_chunk_preview_length] + "..."
                
                # æ›¿æ¢æ¢è¡Œç¬¦ä»¥ä¾¿äºæ§åˆ¶å°æ˜¾ç¤º
                content_preview = content_preview.replace("\n", "\\n")
                
                logger.info(f"\n  å— {i+1}/{len(file_chunks_list[:3])} [ç¬¬{page_num}é¡µ, {chunk_size}å­—ç¬¦]:")
                logger.info(f"  {content_preview}")
            
            # å¦‚æœæ–‡ä»¶ä¸­çš„å—æ•°è¶…è¿‡3ä¸ªï¼Œæ˜¾ç¤ºçœç•¥ä¿¡æ¯
            if len(file_chunks_list) > 3:
                logger.info(f"  ... è¿˜æœ‰ {len(file_chunks_list) - 3} ä¸ªå—æœªæ˜¾ç¤º ...")
                
        # è¾“å‡ºä¿å­˜å®Œæ•´åˆ†å—å†…å®¹çš„æç¤º
        chunks_detail_file = self.cache_dir / "chunks_detail.txt"
        try:
            with open(chunks_detail_file, "w", encoding="utf-8") as f:
                for i, chunk in enumerate(chunks):
                    source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
                    file_name = Path(source).name if isinstance(source, str) else "æœªçŸ¥æ–‡ä»¶"
                    page_num = chunk.metadata.get("page", "æœªçŸ¥é¡µç ")
                    
                    f.write(f"=== å— {i+1}/{len(chunks)} [{file_name} - ç¬¬{page_num}é¡µ] ===\n")
                    f.write(chunk.page_content)
                    f.write("\n\n")
            
            logger.info(f"\nâœ… æ‰€æœ‰æ–‡æœ¬å—çš„è¯¦ç»†å†…å®¹å·²ä¿å­˜è‡³: {chunks_detail_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜è¯¦ç»†å—å†…å®¹å¤±è´¥: {str(e)}")
        
        logger.info("="*50)

    def create_embeddings(self) -> HuggingFaceEmbeddings:
        """åˆ›å»ºåµŒå…¥æ¨¡å‹å®ä¾‹"""
        logger.info("åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        return HuggingFaceEmbeddings(
            model_name=self.config.embedding_model_path,  # åµŒå…¥æ¨¡å‹çš„è·¯å¾„
            model_kwargs={"device": self.config.device},  # è®¾ç½®è®¾å¤‡ä¸ºCPUæˆ–GPU
            encode_kwargs={
                "batch_size": self.config.batch_size,  # æ‰¹å¤„ç†å¤§å°
                "normalize_embeddings": self.config.normalize_embeddings  # æ˜¯å¦å½’ä¸€åŒ–åµŒå…¥
            },
        )

    def backup_vector_db(self):
        """å¤‡ä»½ç°æœ‰å‘é‡æ•°æ®åº“"""
        vector_db_path = Path(self.config.vector_db_path)
        if not vector_db_path.exists():
            return False
            
        try:
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            backup_dir = vector_db_path.parent / f"{vector_db_path.name}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶æ‰€æœ‰æ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•
            for item in vector_db_path.glob('*'):
                if item.is_file():
                    shutil.copy2(item, backup_dir)
                elif item.is_dir():
                    shutil.copytree(item, backup_dir / item.name)
                    
            logger.info(f"âœ… å‘é‡æ•°æ®åº“å·²å¤‡ä»½è‡³ {backup_dir}")
            return True
        except Exception as e:
            logger.error(f"å¤‡ä»½å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            return False

    def build_vector_store(self):
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        logger.info("å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“")

        # åˆ›å»ºå¿…è¦ç›®å½•
        Path(self.config.vector_db_path).mkdir(parents=True, exist_ok=True)

        # å¤„ç†æ–‡æ¡£
        chunks = self.process_files()  # å¤„ç†æ–‡æ¡£å¹¶åˆ†å—
        
        if not chunks:
            logger.warning("æ²¡æœ‰æ–‡æ¡£å—å¯ä»¥å¤„ç†ï¼Œè·³è¿‡å‘é‡å­˜å‚¨æ„å»º")
            return

        # å¤‡ä»½ç°æœ‰å‘é‡æ•°æ®åº“
        if Path(self.config.vector_db_path).exists() and any(Path(self.config.vector_db_path).glob('*')):
            self.backup_vector_db()

        # ç”ŸæˆåµŒå…¥æ¨¡å‹
        embeddings = self.create_embeddings()

        # æ„å»ºå‘é‡å­˜å‚¨
        logger.info("ç”Ÿæˆå‘é‡...")
        # æ„å»ºå‘é‡å­˜å‚¨æ—¶æ˜¾å¼æŒ‡å®š
        vector_store = FAISS.from_documents(
            chunks,
            embeddings,
            distance_strategy=DistanceStrategy.COSINE  # æ˜ç¡®æŒ‡å®šä½™å¼¦ç›¸ä¼¼åº¦
        )

        # ä¿å­˜å‘é‡æ•°æ®åº“
        vector_store.save_local(str(self.config.vector_db_path))  # ä¿å­˜å‘é‡å­˜å‚¨åˆ°æŒ‡å®šè·¯å¾„
        logger.info(f"å‘é‡æ•°æ®åº“å·²ä¿å­˜è‡³ {self.config.vector_db_path}")  # è¾“å‡ºä¿å­˜è·¯å¾„

    def save_chunks_to_file(self, chunks: List[Document]):
        """å°†æ–‡æ¡£åˆ†å—ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼Œä½†ä¸ä½œä¸ºç¼“å­˜å­˜å‚¨
        
        Args:
            chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨
        """
        if not chunks:
            logger.info("æ²¡æœ‰æ–‡æœ¬å—å¯ä¾›ä¿å­˜")
            return
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºçº¯æ–‡æœ¬æ ¼å¼ï¼Œæ–¹ä¾¿ç›´æ¥æŸ¥çœ‹
        text_file = self.cache_dir / "chunks_text.txt"
        try:
            with open(text_file, "w", encoding="utf-8") as f:
                f.write(f"æ–‡æ¡£åˆ†å—æ€»è§ˆ\n")
                f.write(f"=============\n")
                f.write(f"æ€»å—æ•°: {len(chunks)}\n")
                f.write(f"æ¶‰åŠæ–‡ä»¶æ•°: {len(set(chunk.metadata.get('source', '') for chunk in chunks))}\n\n")
                
                # æŒ‰æ–‡ä»¶åˆ†ç»„è¾“å‡º
                file_chunks = {}
                for chunk in chunks:
                    source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
                    if source not in file_chunks:
                        file_chunks[source] = []
                    file_chunks[source].append(chunk)
                
                for file_path, file_chunks_list in sorted(file_chunks.items(), key=lambda x: len(x[1]), reverse=True):
                    file_name = Path(file_path).name if isinstance(file_path, str) else "æœªçŸ¥æ–‡ä»¶"
                    f.write(f"\n{'='*80}\n")
                    f.write(f"æ–‡ä»¶: {file_name} (å…±{len(file_chunks_list)}å—)\n")
                    f.write(f"{'='*80}\n\n")
                    
                    for i, chunk in enumerate(file_chunks_list):
                        # è·å–ç« èŠ‚ä¿¡æ¯
                        section_num = chunk.metadata.get("section_num", "")
                        section_title = chunk.metadata.get("section_title", "")
                        chunk_index = chunk.metadata.get("chunk_index", i)
                        total_chunks = chunk.metadata.get("total_chunks", len(file_chunks_list))
                        position = chunk.metadata.get("position", "")
                        chunk_type = chunk.metadata.get("chunk_type", "")
                        
                        # æ„å»ºå—æ ‡é¢˜
                        header = f"----- å— {chunk_index+1}/{total_chunks} "
                        if section_num and section_title:
                            header += f"[ç« èŠ‚: {section_num} {section_title}, "
                        header += f"ä½ç½®:{position}, {len(chunk.page_content)}å­—ç¬¦"
                        if chunk_type:
                            header += f", ç±»å‹:{chunk_type}"
                        header += "] -----\n"
                        
                        # å†™å…¥å—ä¿¡æ¯
                        f.write(header)
                        f.write(chunk.page_content)
                        f.write("\n\n")
            
            logger.info(f"âœ… æ–‡æœ¬æ ¼å¼çš„åˆ†å—å†…å®¹å·²ä¿å­˜è‡³: {text_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡æœ¬æ ¼å¼çš„åˆ†å—å†…å®¹å¤±è´¥: {str(e)}")
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ŒåŒ…å«å®Œæ•´çš„å…ƒæ•°æ®
        json_file = self.cache_dir / "chunks_detail.json"
        try:
            chunks_data = []
            for i, chunk in enumerate(chunks):
                chunk_data = {
                    "index": i,
                    "content": chunk.page_content,
                    "length": len(chunk.page_content),
                    "metadata": chunk.metadata
                }
                chunks_data.append(chunk_data)
                
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump({
                    "total_chunks": len(chunks),
                    "timestamp": datetime.now().isoformat(),
                    "chunks": chunks_data
                }, f, ensure_ascii=False, indent=2)
                
            logger.info(f"âœ… JSONæ ¼å¼çš„åˆ†å—è¯¦ç»†ä¿¡æ¯å·²ä¿å­˜è‡³: {json_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ ¼å¼çš„åˆ†å—è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}")
        
        # ä¿å­˜CSVæ ¼å¼çš„æ‘˜è¦ä¿¡æ¯ï¼Œæ–¹ä¾¿å¯¼å…¥ç”µå­è¡¨æ ¼æŸ¥çœ‹
        csv_file = self.cache_dir / "chunks_summary.csv"
        try:
            with open(csv_file, "w", encoding="utf-8") as f:
                # å†™å…¥CSVå¤´
                f.write("ç´¢å¼•,æ–‡ä»¶å,ç« èŠ‚ç¼–å·,ç« èŠ‚æ ‡é¢˜,å—ç´¢å¼•,æ€»å—æ•°,ä½ç½®,å­—ç¬¦æ•°,å†…å®¹é¢„è§ˆ\n")
                
                for i, chunk in enumerate(chunks):
                    source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
                    file_name = Path(source).name if isinstance(source, str) else "æœªçŸ¥æ–‡ä»¶"
                    section_num = chunk.metadata.get("section_num", "")
                    section_title = chunk.metadata.get("section_title", "")
                    chunk_index = chunk.metadata.get("chunk_index", i)
                    total_chunks = chunk.metadata.get("total_chunks", 0)
                    position = chunk.metadata.get("position", "")
                    length = len(chunk.page_content)
                    
                    # å†…å®¹é¢„è§ˆï¼Œå»é™¤æ¢è¡Œç¬¦
                    preview = chunk.page_content[:100].replace("\n", " ").replace("\r", " ")
                    if len(chunk.page_content) > 100:
                        preview += "..."
                    preview = f'"{preview}"'  # ç”¨å¼•å·åŒ…å›´ï¼Œé¿å…CSVè§£æé”™è¯¯
                    
                    f.write(f"{i},{file_name},{section_num},{section_title},{chunk_index},{total_chunks},{position},{length},{preview}\n")
                
            logger.info(f"âœ… CSVæ ¼å¼çš„åˆ†å—æ‘˜è¦å·²ä¿å­˜è‡³: {csv_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜CSVæ ¼å¼çš„åˆ†å—æ‘˜è¦å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    try:
        # åˆå§‹åŒ–é…ç½®
        config = Config()
        
        # æ·»åŠ : è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå…è®¸ç”¨æˆ·æŒ‡å®šæ˜¯å¦æ‰“å°è¯¦ç»†åˆ†å—å†…å®¹
        import argparse
        parser = argparse.ArgumentParser(description='æ„å»ºåŒ–å·¥å®‰å…¨é¢†åŸŸå‘é‡æ•°æ®åº“')
        parser.add_argument('--detailed-chunks', action='store_true', 
                           help='æ˜¯å¦è¾“å‡ºè¯¦ç»†çš„åˆ†å—å†…å®¹')
        parser.add_argument('--max-preview', type=int, default=510,
                           help='è¯¦ç»†è¾“å‡ºæ—¶æ¯ä¸ªæ–‡æœ¬å—æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°')
        args = parser.parse_args()
        
        # æ›´æ–°é…ç½®
        if args.detailed_chunks:
            config.print_detailed_chunks = True
            config.max_chunk_preview_length = args.max_preview
            print(f"å°†è¾“å‡ºè¯¦ç»†åˆ†å—å†…å®¹ï¼Œæ¯å—æœ€å¤šæ˜¾ç¤º {args.max_preview} å­—ç¬¦")

        # æ„å»ºå‘é‡æ•°æ®åº“
        builder = VectorDBBuilder(config)
        builder.build_vector_store()

    except Exception as e:
        logger.exception("ç¨‹åºè¿è¡Œå‡ºé”™")  # è®°å½•ç¨‹åºå¼‚å¸¸
    finally:
        logger.info("ç¨‹åºè¿è¡Œç»“æŸ")  # ç¨‹åºç»“æŸæ—¥å¿—
