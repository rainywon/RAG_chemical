# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import json
import logging  # æ—¥å¿—è®°å½•æ¨¡å—
from pathlib import Path  # è·¯å¾„å¤„ç†åº“
from typing import Generator, Optional, List, Tuple, Dict, Any  # ç±»å‹æç¤ºæ”¯æŒ
import warnings  # è­¦å‘Šå¤„ç†
import torch  # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
from langchain_community.vectorstores import FAISS  # FAISSå‘é‡æ•°æ®åº“é›†æˆ
from langchain_core.documents import Document  # æ–‡æ¡£å¯¹è±¡å®šä¹‰
from langchain_core.embeddings import Embeddings  # åµŒå…¥æ¨¡å‹æ¥å£
from langchain_ollama import OllamaLLM  # Ollamaè¯­è¨€æ¨¡å‹é›†æˆ
from rank_bm25 import BM25Okapi  # BM25æ£€ç´¢ç®—æ³•
from transformers import AutoModelForSequenceClassification, AutoTokenizer  # Transformeræ¨¡å‹
from config import Config  # è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
from build_vector_store import VectorDBBuilder  # å‘é‡æ•°æ®åº“æ„å»ºå™¨
import numpy as np  # æ•°å€¼è®¡ç®—åº“
import pickle  # ç”¨äºåºåˆ—åŒ–å¯¹è±¡
import hashlib  # ç”¨äºç”Ÿæˆå“ˆå¸Œå€¼
import time  # æ·»åŠ timeæ¨¡å—ç”¨äºæ—¶é—´æµ‹é‡
import re  # ç”¨äºæ­£åˆ™è¡¨è¾¾å¼å¤„ç†

# æå‰åˆå§‹åŒ–jiebaï¼ŒåŠ å¿«åç»­å¯åŠ¨é€Ÿåº¦
import os
import jieba  # ä¸­æ–‡åˆ†è¯åº“

# è®¾ç½®jiebaæ—¥å¿—çº§åˆ«ï¼Œå‡å°‘è¾“å‡º
jieba.setLogLevel(logging.INFO)

# é¢„åŠ è½½jiebaåˆ†è¯å™¨
jieba.initialize()

# ç¦ç”¨ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# é…ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨


class RAGSystem:
    """RAGé—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒæ–‡æ¡£æ£€ç´¢å’Œç”Ÿæˆå¼é—®ç­”

    ç‰¹æ€§ï¼š
    - è‡ªåŠ¨ç®¡ç†å‘é‡æ•°æ®åº“ç”Ÿå‘½å‘¨æœŸ
    - æ”¯æŒæµå¼ç”Ÿæˆå’ŒåŒæ­¥ç”Ÿæˆ
    - å¯é…ç½®çš„æ£€ç´¢ç­–ç•¥
    - å®Œå–„çš„é”™è¯¯å¤„ç†
    """

    def __init__(self, config: Config):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ

        :param config: åŒ…å«æ‰€æœ‰é…ç½®å‚æ•°çš„Configå¯¹è±¡
        """
        self.config = config  # ä¿å­˜é…ç½®å¯¹è±¡
        self.vector_store: Optional[FAISS] = None  # FAISSå‘é‡æ•°æ®åº“å®ä¾‹
        self.llm: Optional[OllamaLLM] = None  # Ollamaè¯­è¨€æ¨¡å‹å®ä¾‹
        self.embeddings: Optional[Embeddings] = None  # åµŒå…¥æ¨¡å‹å®ä¾‹
        self.rerank_model = None  # é‡æ’åºæ¨¡å‹
        self.vector_db_build = VectorDBBuilder(config)  # å‘é‡æ•°æ®åº“æ„å»ºå™¨å®ä¾‹
        self._tokenize_cache = {}  # æ·»åŠ åˆ†è¯ç¼“å­˜å­—å…¸

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self._init_logging()  # åˆå§‹åŒ–æ—¥å¿—é…ç½®
        self._init_embeddings()  # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self._init_vector_store()  # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self._init_bm25_retriever()  # åˆå§‹åŒ–BM25æ£€ç´¢å™¨
        self._init_llm()  # åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
        self._init_rerank_model()  # åˆå§‹åŒ–é‡æ’åºæ¨¡å‹

    def _tokenize(self, text: str) -> List[str]:
        """ä¸“ä¸šä¸­æ–‡åˆ†è¯å¤„ç†ï¼Œä½¿ç”¨ç¼“å­˜æé«˜æ€§èƒ½
        :param text: å¾…åˆ†è¯çš„æ–‡æœ¬
        :return: åˆ†è¯åçš„è¯é¡¹åˆ—è¡¨
        """
        # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰ç»“æœ
        if text in self._tokenize_cache:
            return self._tokenize_cache[text]
        
        # å¦‚æœæ–‡æœ¬è¿‡é•¿ï¼Œåªç¼“å­˜å‰2000ä¸ªå­—ç¬¦çš„åˆ†è¯ç»“æœ
        cache_key = text[:2000] if len(text) > 2000 else text
        
        # åˆ†è¯å¤„ç†
        result = [word for word in jieba.cut(text) if word.strip()]
        
        # åªåœ¨ç¼“å­˜ä¸è¶…è¿‡10000ä¸ªæ¡ç›®æ—¶è¿›è¡Œç¼“å­˜
        if len(self._tokenize_cache) < 10000:
            self._tokenize_cache[cache_key] = result
            
        return result

    def _init_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—é…ç½®"""
        logging.basicConfig(
            level=logging.INFO,  # æ—¥å¿—çº§åˆ«è®¾ä¸ºINFO
            format="%(asctime)s - %(levelname)s - %(message)s",  # æ—¥å¿—æ ¼å¼
            handlers=[logging.StreamHandler()]  # è¾“å‡ºåˆ°æ§åˆ¶å°
        )

    def _init_embeddings(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
            # é€šè¿‡æ„å»ºå™¨åˆ›å»ºåµŒå…¥æ¨¡å‹å®ä¾‹
            self.embeddings = self.vector_db_build.create_embeddings()
            logger.info("âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error("âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {str(e)}")

    def _init_vector_store(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        try:
            vector_path = Path(self.config.vector_db_path)  # è·å–å‘é‡åº“è·¯å¾„

            # æ£€æŸ¥ç°æœ‰å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨
            if vector_path.exists():
                logger.info("ğŸ” æ­£åœ¨åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“...")
                if not self.embeddings:
                    raise ValueError("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")

                # åŠ è½½æœ¬åœ°FAISSæ•°æ®åº“
                self.vector_store = FAISS.load_local(
                    folder_path=str(vector_path),
                    embeddings=self.embeddings,
                    allow_dangerous_deserialization=True  # å…è®¸åŠ è½½æ—§ç‰ˆæœ¬åºåˆ—åŒ–æ•°æ®
                )
                logger.info(f"âœ… å·²åŠ è½½å‘é‡æ•°æ®åº“ï¼š{vector_path}")
            else:
                # æ„å»ºæ–°å‘é‡æ•°æ®åº“
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œæ­£åœ¨æ„å»ºæ–°æ•°æ®åº“...")
                self.vector_store = self.vector_db_build.build_vector_store()
                logger.info(f"âœ… æ–°å»ºå‘é‡æ•°æ®åº“å·²ä¿å­˜è‡³ï¼š{vector_path}")
        except Exception as e:
            logger.error("âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–å‘é‡æ•°æ®åº“: {str(e)}")

    def _init_rerank_model(self):
        """åˆå§‹åŒ–é‡æ’åºæ¨¡å‹"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–rerankæ¨¡å‹...")
            # ä»HuggingFaceåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
            self.rerank_model = AutoModelForSequenceClassification.from_pretrained(
                self.config.rerank_model_path
            )
            self.rerank_tokenizer = AutoTokenizer.from_pretrained(self.config.rerank_model_path)
            
            # å°è¯•å°†æ¨¡å‹ç§»è‡³GPUï¼Œå¦‚æœå¯ç”¨
            if torch.cuda.is_available():
                logger.info("ğŸš€ å°†é‡æ’åºæ¨¡å‹ç§»è‡³GPUåŠ é€Ÿ")
                self.rerank_model = self.rerank_model.to("cuda")
                self.using_gpu = True
            else:
                self.using_gpu = False
                
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œæé«˜æ¨ç†é€Ÿåº¦
            self.rerank_model.eval()
            
            # åˆå§‹åŒ–é‡æ’åºç¼“å­˜
            self.rerank_cache = {}
            
            logger.info("âœ… rerankæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ rerankæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–rerankæ¨¡å‹: {str(e)}")

    def _init_llm(self):
        """åˆå§‹åŒ–Ollamaå¤§è¯­è¨€æ¨¡å‹"""
        try:
            logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ–Ollamaæ¨¡å‹...")
            # åˆ›å»ºOllamaLLMå®ä¾‹
            self.llm = OllamaLLM(
                model="deepseek_8B:latest",  # æ¨¡å‹åç§°
                #deepseek_8B:latest   1513b8b198dc    8.5 GB    59 seconds ago
                # deepseek-r1:8b             2deepseek_8B:latest GB    46 minutes ago
                # deepseek-r1:14b            ea35dfe18182    9.0 GB    29 hours ago
                base_url=self.config.ollama_base_url,  # OllamaæœåŠ¡åœ°å€
                temperature=self.config.llm_temperature,  # æ¸©åº¦å‚æ•°æ§åˆ¶éšæœºæ€§
                num_predict=self.config.llm_max_tokens,  # æœ€å¤§ç”Ÿæˆtokenæ•°
                stop=["<|im_end|>"]
            )

            # æµ‹è¯•æ¨¡å‹è¿æ¥
            logger.info("âœ… Ollamaæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ Ollamaæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–Ollamaæ¨¡å‹: {str(e)}")

    def _init_bm25_retriever(self):
        """åˆå§‹åŒ–BM25æ£€ç´¢å™¨ï¼ˆæŒä¹…åŒ–ç¼“å­˜ç‰ˆï¼‰"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–BM25æ£€ç´¢å™¨...")

            # éªŒè¯å‘é‡åº“æ˜¯å¦åŒ…å«æ–‡æ¡£
            if not self.vector_store.docstore._dict:
                raise ValueError("å‘é‡åº“ä¸­æ— å¯ç”¨æ–‡æ¡£")

            # ä»å‘é‡åº“åŠ è½½æ‰€æœ‰æ–‡æ¡£å†…å®¹
            all_docs = list(self.vector_store.docstore._dict.values())
            self.bm25_docs = [doc.page_content for doc in all_docs]
            self.doc_metadata = [doc.metadata for doc in all_docs]
            
            # è®¡ç®—æ–‡æ¡£é›†åˆçš„å“ˆå¸Œå€¼ï¼Œç”¨äºç¼“å­˜æ ‡è¯†
            docs_hash = hashlib.md5(str([d[:100] for d in self.bm25_docs]).encode()).hexdigest()
            cache_path = Path(self.config.vector_db_path).parent / f"bm25_tokenized_cache_{docs_hash}.pkl"
            
            # å°è¯•åŠ è½½ç¼“å­˜çš„åˆ†è¯ç»“æœ
            if cache_path.exists():
                try:
                    logger.info(f"å‘ç°BM25åˆ†è¯ç¼“å­˜ï¼Œæ­£åœ¨åŠ è½½ï¼š{cache_path}")
                    with open(cache_path, 'rb') as f:
                        cached_data = pickle.load(f)
                        tokenized_docs = cached_data.get('tokenized_docs')
                        
                    if tokenized_docs and len(tokenized_docs) == len(self.bm25_docs):
                        logger.info(f"æˆåŠŸåŠ è½½ç¼“å­˜çš„åˆ†è¯ç»“æœï¼Œå…± {len(tokenized_docs)} ç¯‡æ–‡æ¡£")
                    else:
                        logger.warning("ç¼“å­˜æ•°æ®ä¸åŒ¹é…ï¼Œå°†é‡æ–°å¤„ç†åˆ†è¯")
                        tokenized_docs = None
                except Exception as e:
                    logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {str(e)}ï¼Œå°†é‡æ–°å¤„ç†åˆ†è¯")
                    tokenized_docs = None
            else:
                tokenized_docs = None
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ç¼“å­˜ï¼Œé‡æ–°åˆ†è¯å¤„ç†
            if tokenized_docs is None:
                logger.info(f"å¼€å§‹å¤„ç† {len(self.bm25_docs)} ç¯‡æ–‡æ¡£è¿›è¡ŒBM25ç´¢å¼•...")
                
                # æ‰¹å¤„ç†åˆ†è¯ä»¥å‡å°‘å†…å­˜å‹åŠ›
                batch_size = 100  # æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°
                tokenized_docs = []
                
                for i in range(0, len(self.bm25_docs), batch_size):
                    batch = self.bm25_docs[i:i+batch_size]
                    batch_tokenized = [self._tokenize(doc) for doc in batch]
                    tokenized_docs.extend(batch_tokenized)
                    
                    if (i + batch_size) % 500 == 0 or (i + batch_size) >= len(self.bm25_docs):
                        logger.info(f"å·²å¤„ç† {min(i + batch_size, len(self.bm25_docs))}/{len(self.bm25_docs)} ç¯‡æ–‡æ¡£")
                
                # ä¿å­˜åˆ†è¯ç»“æœåˆ°ç¼“å­˜
                try:
                    logger.info(f"ä¿å­˜åˆ†è¯ç»“æœåˆ°ç¼“å­˜ï¼š{cache_path}")
                    with open(cache_path, 'wb') as f:
                        pickle.dump({'tokenized_docs': tokenized_docs}, f)
                except Exception as e:
                    logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {str(e)}")

            # éªŒè¯åˆ†è¯ç»“æœæœ‰æ•ˆæ€§
            if len(tokenized_docs) == 0 or all(len(d) == 0 for d in tokenized_docs):
                raise ValueError("æ–‡æ¡£åˆ†è¯åä¸ºç©ºï¼Œè¯·æ£€æŸ¥åˆ†è¯é€»è¾‘")

            # åˆå§‹åŒ–BM25æ¨¡å‹
            logger.info("å¼€å§‹æ„å»ºBM25ç´¢å¼•...")
            self.bm25 = BM25Okapi(tokenized_docs)

            logger.info(f"âœ… BM25åˆå§‹åŒ–å®Œæˆï¼Œæ–‡æ¡£æ•°ï¼š{len(self.bm25_docs)}")
        except Exception as e:
            logger.error(f"âŒ BM25åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"BM25åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    def _hybrid_retrieve(self, question: str) -> List[Dict[str, Any]]:
        """æ··åˆæ£€ç´¢æµç¨‹ï¼ˆå‘é‡+BM25ï¼‰

        :param question: ç”¨æˆ·é—®é¢˜
        :return: åŒ…å«æ–‡æ¡£å’Œæ£€ç´¢ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        # åŠ¨æ€ç¡®å®šæ£€ç´¢ç­–ç•¥æƒé‡
        vector_weight, bm25_weight = self._determine_retrieval_weights(question)
        logger.info(f"åŠ¨æ€æƒé‡: å‘é‡æ£€ç´¢={vector_weight:.2f}, BM25æ£€ç´¢={bm25_weight:.2f}")

        # ä¸€ã€å‘é‡æ£€ç´¢éƒ¨åˆ†
        # 1. æ‰§è¡Œå‘é‡æ£€ç´¢
        vector_results = self.vector_store.similarity_search_with_score(
            question, k=self.config.vector_top_k
        )
        
        # 2. å¯¹å‘é‡æ£€ç´¢ç»“æœå»é‡å¹¶æ ‡å‡†åŒ–åˆ†æ•°
        unique_vector_results = {}
        for doc, score in vector_results:
            doc_id = doc.metadata.get("source", "") + str(hash(doc.page_content))
            norm_score = (score + 1) / 2  # è½¬æ¢ä¸ºæ ‡å‡†ä½™å¼¦å€¼ï¼ˆ0~1èŒƒå›´ï¼‰
            
            # å¦‚æœæ–‡æ¡£å·²å­˜åœ¨ä¸”æ–°åˆ†æ•°æ›´é«˜ï¼Œåˆ™æ›´æ–°
            if doc_id not in unique_vector_results or norm_score > unique_vector_results[doc_id][1]:
                unique_vector_results[doc_id] = (doc, norm_score)
        
        # 3. è¿‡æ»¤ä½åˆ†ç»“æœå¹¶åº”ç”¨æƒé‡
        filtered_vector_results = []
        for doc, score in unique_vector_results.values():
            if score >= self.config.vector_similarity_threshold:
                # åº”ç”¨å‘é‡æ£€ç´¢æƒé‡
                weighted_score = score * vector_weight
                filtered_vector_results.append({
                    "doc": doc,
                    "score": weighted_score,  # åº”ç”¨æƒé‡åçš„åˆ†æ•°
                    "raw_score": score,       # åŸå§‹åˆ†æ•°
                    "type": "vector",
                    "source": doc.metadata.get("source", "unknown")
                })

        # äºŒã€BM25æ£€ç´¢éƒ¨åˆ†
        # 1. é—®é¢˜åˆ†è¯å¹¶è®¡ç®—BM25åˆ†æ•°
        tokenized_query = self._tokenize(question)
        bm25_scores = self.bm25.get_scores(tokenized_query)
        
        # 2. è·å–top kçš„BM25ç»“æœ
        top_bm25_indices = np.argsort(bm25_scores)[-self.config.bm25_top_k:][::-1]
        top_bm25_scores = [bm25_scores[idx] for idx in top_bm25_indices]
        
        # 3. å¯¹BM25åˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
        normalized_bm25_scores = []
        if top_bm25_scores:
            # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
            mean_score = np.mean(top_bm25_scores)
            std_score = np.std(top_bm25_scores) + 1e-9  # é¿å…é™¤ä»¥0
            
            # ä½¿ç”¨Logisticå½’ä¸€åŒ–
            for score in top_bm25_scores:
                z_score = (score - mean_score) / std_score  # Z-scoreæ ‡å‡†åŒ–
                logistic_score = 1 / (1 + np.exp(-z_score))  # Sigmoidå‡½æ•°
                normalized_bm25_scores.append(logistic_score)
        
        # 4. æ„å»ºBM25æ£€ç´¢ç»“æœå¹¶åº”ç”¨æƒé‡
        filtered_bm25_results = []
        for idx, norm_score in zip(top_bm25_indices, normalized_bm25_scores):
            if norm_score >= self.config.bm25_similarity_threshold:
                doc = Document(
                    page_content=self.bm25_docs[idx],
                    metadata=self.doc_metadata[idx]
                )
                # åº”ç”¨BM25æ£€ç´¢æƒé‡
                weighted_score = norm_score * bm25_weight
                filtered_bm25_results.append({
                    "doc": doc,
                    "score": weighted_score,  # åº”ç”¨æƒé‡åçš„åˆ†æ•°
                    "raw_score": norm_score,  # åŸå§‹åˆ†æ•°
                    "type": "bm25",
                    "source": doc.metadata.get("source", "unknown")
                })

        # åˆå¹¶ä¸¤ç§æ£€ç´¢çš„ç»“æœ
        results = filtered_vector_results + filtered_bm25_results
        logger.info(f"ğŸ“š æ··åˆæ£€ç´¢åå¾—åˆ°{len(results)}ç¯‡æ–‡æ¡£")
        return results
    
    def _determine_retrieval_weights(self, question: str) -> Tuple[float, float]:
        """åŠ¨æ€ç¡®å®šæ£€ç´¢ç­–ç•¥æƒé‡
        
        æ ¹æ®é—®é¢˜çš„ç‰¹å¾å’Œé¢†åŸŸçŸ¥è¯†åŠ¨æ€è°ƒæ•´å‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢çš„æƒé‡ï¼Œ
        æé«˜æ··åˆæ£€ç´¢çš„é€‚ç”¨æ€§å’Œå‡†ç¡®æ€§
        
        :param question: ç”¨æˆ·é—®é¢˜
        :return: (å‘é‡æ£€ç´¢æƒé‡, BM25æ£€ç´¢æƒé‡)
        """
        # é»˜è®¤æƒé‡
        default_vector = 0.5
        default_bm25 = 0.5
        
        try:
            # 1. é—®é¢˜ç±»å‹ç‰¹å¾åˆ†æ
            # äº‹å®å‹é—®é¢˜ç‰¹å¾è¯ï¼ˆåå‘BM25ï¼‰- ç²¾ç¡®åŒ¹é…æ›´æœ‰æ•ˆ
            factual_indicators = [
                'ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'å¦‚ä½•', 'æ€ä¹ˆ', 'å“ªäº›', 'è°', 'ä½•æ—¶', 'ä¸ºä»€ä¹ˆ', 
                'å¤šå°‘', 'æ•°æ®', 'æ ‡å‡†æ˜¯', 'è¦æ±‚æ˜¯', 'åˆ—ä¸¾', 'æ­¥éª¤', 'æ–¹æ³•',
                'æµç¨‹', 'è§„å®š', 'åœ°ç‚¹', 'æ—¶é—´', 'å“ªé‡Œ', 'è§„èŒƒ', 'æ¡ä¾‹'
            ]
            
            # æ¦‚å¿µå‹é—®é¢˜ç‰¹å¾è¯ï¼ˆåå‘å‘é‡æ£€ç´¢ï¼‰- è¯­ä¹‰ç†è§£æ›´æœ‰æ•ˆ
            conceptual_indicators = [
                'è§£é‡Š', 'åˆ†æ', 'è¯„ä»·', 'æ¯”è¾ƒ', 'åŒºåˆ«', 'å…³ç³»', 'å½±å“', 'åŸç†', 
                'æœºåˆ¶', 'æ€è€ƒ', 'å¯èƒ½', 'å»ºè®®', 'é¢„æµ‹', 'æ¨æµ‹', 'è¯„ä¼°', 'ä¼˜ç¼ºç‚¹',
                'æ„ä¹‰', 'ä»·å€¼', 'è”ç³»', 'çœ‹æ³•', 'è§‚ç‚¹', 'ç†è§£', 'è®¤ä¸º'
            ]
            
            # åŒ–å·¥å®‰å…¨ç‰¹å®šæœ¯è¯­ï¼ˆå¢åŠ é¢†åŸŸç‰¹å¼‚æ€§ï¼‰
            chemical_safety_terms = [
                'åŒ–å­¦å“', 'æ˜“ç‡ƒ', 'æ˜“çˆ†', 'æœ‰æ¯’', 'è…èš€', 'å±é™©', 'å®‰å…¨', 'é˜²æŠ¤', 
                'äº‹æ•…', 'æ³„æ¼', 'çˆ†ç‚¸', 'ç«ç¾', 'ä¸­æ¯’', 'åº”æ€¥', 'å¤„ç½®', 'é£é™©',
                'å±å®³', 'é˜²èŒƒ', 'æªæ–½', 'æ“ä½œ', 'ååº”', 'ç‰©è´¨', 'æ°”ä½“', 'æ¶²ä½“', 
                'å›ºä½“', 'æµ“åº¦', 'æ¸©åº¦', 'å‹åŠ›', 'å‚¨å­˜', 'è¿è¾“'
            ]
            
            # 2. å¤šç»´åº¦ç‰¹å¾æå–
            # è®¡ç®—é—®é¢˜ç±»å‹ç‰¹å¾å‡ºç°æ¬¡æ•°å’Œå¼ºåº¦
            factual_count = sum(1 for term in factual_indicators if term in question)
            conceptual_count = sum(1 for term in conceptual_indicators if term in question)
            domain_term_count = sum(1 for term in chemical_safety_terms if term in question)
            
            # é—®é¢˜é•¿åº¦å› ç´ ï¼ˆè¾ƒé•¿é—®é¢˜é€šå¸¸åå‘è¯­ä¹‰ç†è§£ï¼‰
            query_length = len(question)
            length_factor = min(1.0, query_length / 50)  # æ ‡å‡†åŒ–é•¿åº¦å› ç´ 
            
            # é—®é¢˜å¤æ‚åº¦å› ç´ ï¼ˆå¥å­ç»“æ„å¤æ‚åº¦ï¼‰
            sentence_count = len([s for s in re.split(r'[ã€‚ï¼Ÿï¼.?!]', question) if s.strip()])
            complexity_factor = min(1.0, sentence_count / 3)  # æ ‡å‡†åŒ–å¤æ‚åº¦å› ç´ 
            
            # æ•°å­—å’Œç¬¦å·æ•°é‡ï¼ˆæ›´å¤šæ•°å­—é€šå¸¸åå‘ç²¾ç¡®åŒ¹é…ï¼‰
            digit_count = sum(1 for c in question if c.isdigit())
            digit_factor = min(1.0, digit_count / 5)
            
            # 3. ç‰¹å¾æ•´åˆä¸æƒé‡è®¡ç®—
            vector_weight = default_vector
            bm25_weight = default_bm25
            
            # åŸºç¡€æƒé‡è°ƒæ•´
            if factual_count > conceptual_count:
                # äº‹å®å‹é—®é¢˜ï¼šå¢åŠ BM25æƒé‡
                bm25_base = 0.6 + 0.1 * min(factual_count, 3)
            elif conceptual_count > factual_count:
                # æ¦‚å¿µå‹é—®é¢˜ï¼šå¢åŠ å‘é‡æƒé‡
                vector_base = 0.6 + 0.1 * min(conceptual_count, 3)
                bm25_base = 1.0 - vector_base
            else:
                # æ··åˆç±»å‹é—®é¢˜ï¼šæ ¹æ®é•¿åº¦å¾®è°ƒ
                vector_base = default_vector
                bm25_base = default_bm25
            
            # åº”ç”¨ä¿®æ­£å› å­
            vector_modifiers = [
                0.1 * length_factor,         # é—®é¢˜é•¿åº¦ä¿®æ­£
                0.1 * complexity_factor,     # å¤æ‚åº¦ä¿®æ­£
                -0.1 * digit_factor,         # æ•°å­—å› ç´ ä¿®æ­£(å‡å°‘å‘é‡æƒé‡)
                0.05 * min(domain_term_count, 4) / 4  # é¢†åŸŸæœ¯è¯­ä¿®æ­£
            ]
            
            # è®¡ç®—æœ€ç»ˆæƒé‡
            if factual_count > conceptual_count:
                # äº‹å®å‹é—®é¢˜
                bm25_weight = bm25_base
                # å¯¹BM25æƒé‡åº”ç”¨å°å¹…ä¿®æ­£
                for modifier in vector_modifiers:
                    bm25_weight -= modifier / 2  # å‡å°ä¿®æ­£å› å­å½±å“
                vector_weight = 1.0 - bm25_weight
            else:
                # æ¦‚å¿µå‹é—®é¢˜æˆ–æ··åˆå‹é—®é¢˜
                vector_weight = vector_base
                # åº”ç”¨å®Œæ•´ä¿®æ­£å› å­
                for modifier in vector_modifiers:
                    vector_weight += modifier
                bm25_weight = 1.0 - vector_weight
            
            # è¾¹ç•Œçº¦æŸ
            vector_weight = max(0.2, min(0.8, vector_weight))  # é™åˆ¶åœ¨0.2-0.8èŒƒå›´å†…
            bm25_weight = 1.0 - vector_weight
            
            # ç¡®ä¿æƒé‡å’Œä¸º1
            total = vector_weight + bm25_weight
            normalized_vector = vector_weight / total
            normalized_bm25 = bm25_weight / total
            
            logger.debug(f"é—®é¢˜ç‰¹å¾åˆ†æ: äº‹å®å‹={factual_count}, æ¦‚å¿µå‹={conceptual_count}, é¢†åŸŸæœ¯è¯­={domain_term_count}, é•¿åº¦={query_length}, å¤æ‚åº¦={sentence_count}, æ•°å­—={digit_count}")
            
            return normalized_vector, normalized_bm25
            
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ¨æ€æƒé‡è®¡ç®—å¤±è´¥: {str(e)}")
            return default_vector, default_bm25

    def _rerank_documents(self, results: List[Dict], question: str) -> List[Dict]:
        """ä½¿ç”¨é‡æ’åºæ¨¡å‹ä¼˜åŒ–æ£€ç´¢ç»“æœ

        :param results: æ£€ç´¢ç»“æœåˆ—è¡¨
        :param question: åŸå§‹é—®é¢˜
        :return: é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        try:
            if not results:
                return results

            # æ‰¹å¤„ç†é€»è¾‘ï¼Œæ¯æ¬¡å¤„ç†å°‘é‡æ–‡æ¡£
            batch_size = 8  # å‡å°æ‰¹å¤„ç†å¤§å°ä»¥é¿å…å¼ é‡ç»´åº¦ä¸åŒ¹é…
            batched_rerank_scores = []
            
            # é™åˆ¶æ–‡æ¡£é•¿åº¦ï¼Œé¿å…è¿‡é•¿æ–‡æ¡£
            max_doc_length = 5000  # è®¾ç½®æœ€å¤§æ–‡æ¡£é•¿åº¦
            for res in results:
                if len(res["doc"].page_content) > max_doc_length:
                    res["doc"].page_content = res["doc"].page_content[:max_doc_length]
            
            # åˆ†æ‰¹å¤„ç†æ–‡æ¡£
            for i in range(0, len(results), batch_size):
                batch_results = results[i:i+batch_size]
                batch_pairs = [(question, res["doc"].page_content) for res in batch_results]
                
                try:
                    # å¯¹è¾“å…¥è¿›è¡Œtokenizeå’Œæ‰¹å¤„ç†
                    batch_inputs = self.rerank_tokenizer(
                        batch_pairs,
                        padding=True,
                        truncation=True,
                        max_length=512,  # é™åˆ¶ç»Ÿä¸€çš„æœ€å¤§é•¿åº¦
                        return_tensors="pt"
                    )
                    
                    # ç¡®ä¿å¼ é‡åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                    if self.using_gpu and torch.cuda.is_available():
                        batch_inputs = {k: v.to("cuda") for k, v in batch_inputs.items()}
                    
                    # æ¨¡å‹æ¨ç†
                    with torch.no_grad():
                        batch_outputs = self.rerank_model(**batch_inputs)
                        # ä½¿ç”¨sigmoidè½¬æ¢åˆ†æ•°
                        batch_scores = torch.sigmoid(batch_outputs.logits).squeeze().cpu().tolist()
                        
                        # ç¡®ä¿batch_scoresæ˜¯åˆ—è¡¨
                        if not isinstance(batch_scores, list):
                            batch_scores = [batch_scores]
                        
                        batched_rerank_scores.extend(batch_scores)
                except Exception as e:
                    # æ‰¹å¤„ç†å¤±è´¥æ—¶ï¼Œä½¿ç”¨åŸå§‹åˆ†æ•°
                    logger.warning(f"æ–‡æ¡£æ‰¹æ¬¡ {i//batch_size+1} é‡æ’åºå¤±è´¥: {str(e)}")
                    for res in batch_results:
                        batched_rerank_scores.append(res["score"])

            # æ›´æ–°ç»“æœåˆ†æ•°
            for res, rerank_score in zip(results, batched_rerank_scores):
                # ç›´æ¥ä½¿ç”¨é‡æ’åºåˆ†æ•°ä½œä¸ºæœ€ç»ˆåˆ†æ•°
                res.update({
                    "original_score": res["score"],  # ä¿å­˜åŸå§‹æ£€ç´¢åˆ†æ•°
                    "rerank_score": rerank_score,
                    "final_score": rerank_score  # ç›´æ¥ä½¿ç”¨é‡æ’åºåˆ†æ•°ä½œä¸ºæœ€ç»ˆåˆ†æ•°
                })
                
                # è®°å½•æ—¥å¿—
                logger.debug(f"æ–‡æ¡£é‡æ’åº: {res['source']} - åŸå§‹åˆ†æ•°: {res['original_score']:.4f} - é‡æ’åºåˆ†æ•°: {rerank_score:.4f}")

            # æŒ‰æœ€ç»ˆåˆ†æ•°é™åºæ’åˆ—
            sorted_results = sorted(results, key=lambda x: x["final_score"], reverse=True)
            
            # åº”ç”¨å¤šæ ·æ€§å¢å¼ºç­–ç•¥
            return self._diversify_results(sorted_results)
            
        except Exception as e:
            logger.error(f"é‡æ’åºæ•´ä½“å¤±è´¥: {str(e)}")
            # ç¡®ä¿æ¯ä¸ªç»“æœéƒ½æœ‰å¿…è¦çš„å­—æ®µ
            for res in results:
                if "final_score" not in res:
                    res["final_score"] = res["score"]
                if "rerank_score" not in res:
                    res["rerank_score"] = res["score"]
                if "original_score" not in res:
                    res["original_score"] = res["score"]
            
            # è¿”å›åŸå§‹æ’åºçš„ç»“æœ
            return sorted(results, key=lambda x: x["score"], reverse=True)
    
    def _diversify_results(self, ranked_results: List[Dict]) -> List[Dict]:
        """å¢å¼ºæ£€ç´¢ç»“æœçš„å¤šæ ·æ€§
        
        ä½¿ç”¨MMR(Maximum Marginal Relevance)ç®—æ³•å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§
        
        :param ranked_results: æŒ‰åˆ†æ•°æ’åºçš„æ£€ç´¢ç»“æœ
        :return: å¤šæ ·æ€§å¢å¼ºåçš„ç»“æœ
        """
        if len(ranked_results) <= 2:
            return ranked_results  # ç»“æœå¤ªå°‘ä¸éœ€è¦å¤šæ ·æ€§ä¼˜åŒ–
        
        try:
            # MMRå‚æ•°
            lambda_param = 0.7  # æ§åˆ¶ç›¸å…³æ€§vså¤šæ ·æ€§çš„å¹³è¡¡ï¼Œè¶Šå¤§è¶Šåå‘ç›¸å…³æ€§
            
            # åˆå§‹åŒ–å·²é€‰æ‹©å’Œå€™é€‰æ–‡æ¡£
            selected = [ranked_results[0]]  # æœ€é«˜åˆ†æ–‡æ¡£ç›´æ¥é€‰å…¥
            candidates = ranked_results[1:]
            
            # å¤„ç†top 20æ–‡æ¡£
            while len(selected) < min(len(ranked_results), self.config.final_top_k):
                # è®¡ç®—æ¯ä¸ªå€™é€‰æ–‡æ¡£çš„MMRåˆ†æ•°
                mmr_scores = []
                
                for candidate in candidates:
                    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆç›¸å…³æ€§éƒ¨åˆ†ï¼‰
                    relevance = candidate["final_score"]
                    
                    # è®¡ç®—ä¸å·²é€‰æ–‡æ¡£çš„æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆå¤šæ ·æ€§éƒ¨åˆ†ï¼‰
                    max_sim = 0
                    for selected_doc in selected:
                        # ä½¿ç”¨æ–‡æœ¬å†…å®¹çš„è¯é‡å è®¡ç®—ç›¸ä¼¼åº¦
                        sim = self._compute_document_similarity(
                            candidate["doc"].page_content,
                            selected_doc["doc"].page_content
                        )
                        max_sim = max(max_sim, sim)
                    
                    # è®¡ç®—MMRåˆ†æ•°
                    mmr = lambda_param * relevance - (1 - lambda_param) * max_sim
                    mmr_scores.append(mmr)
                
                # é€‰æ‹©MMRåˆ†æ•°æœ€é«˜çš„æ–‡æ¡£
                best_idx = mmr_scores.index(max(mmr_scores))
                selected.append(candidates.pop(best_idx))
            
            # è¿”å›å¤šæ ·æ€§å¢å¼ºåçš„æ–‡æ¡£
            return selected
            
        except Exception as e:
            logger.error(f"å¤šæ ·æ€§å¢å¼ºå¤±è´¥: {str(e)}")
            # å¤±è´¥æ—¶è¿”å›åŸå§‹æ’åºçš„å‰20ä¸ªæ–‡æ¡£
            return ranked_results[:self.config.final_top_k]
    
    def _compute_document_similarity(self, doc1: str, doc2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦
        
        :param doc1: ç¬¬ä¸€ä¸ªæ–‡æ¡£å†…å®¹
        :param doc2: ç¬¬äºŒä¸ªæ–‡æ¡£å†…å®¹
        :return: ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        try:
            # ä½¿ç”¨åŸºäºè¯é›†åˆçš„Jaccardç›¸ä¼¼åº¦
            tokens1 = set(self._tokenize(doc1))
            tokens2 = set(self._tokenize(doc2))
            
            # è®¡ç®—Jaccardç³»æ•°
            if not tokens1 or not tokens2:
                return 0.0
                
            intersection = tokens1.intersection(tokens2)
            union = tokens1.union(tokens2)
            
            # å¦‚æœæ–‡æ¡£é•¿åº¦ç›¸å·®å¤ªå¤§ï¼Œç»™äºˆæƒ©ç½š
            len_ratio = min(len(doc1), len(doc2)) / max(len(doc1), len(doc2))
            
            # åŠ æƒç›¸ä¼¼åº¦
            return (len(intersection) / len(union)) * len_ratio
            
        except Exception as e:
            logger.warning(f"æ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}")
            return 0.0

    def _retrieve_documents(self, question: str, use_rerank: bool = True) -> Tuple[List[Document], List[Dict]]:
        """å®Œæ•´æ£€ç´¢æµç¨‹

        :param question: ç”¨æˆ·é—®é¢˜
        :param use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åºï¼Œé»˜è®¤ä¸ºTrue
        :return: (æ–‡æ¡£åˆ—è¡¨, åˆ†æ•°ä¿¡æ¯åˆ—è¡¨)
        """
        try:
            # å¼€å§‹è®¡æ—¶
            start_time = time.time()
            
            # æ··åˆæ£€ç´¢
            hybrid_start = time.time()
            raw_results = self._hybrid_retrieve(question)
            hybrid_time = time.time() - hybrid_start
            
            if not raw_results:
                logger.warning("æ··åˆæ£€ç´¢æœªè¿”å›ä»»ä½•ç»“æœ")
                return [], []

            # é‡æ’åº(å¯è·³è¿‡)
            rerank_time = 0
            if use_rerank:
                rerank_start = time.time()
                try:
                    reranked = self._rerank_documents(raw_results, question)
                except Exception as e:
                    logger.error(f"é‡æ’åºå®Œå…¨å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç»“æœ: {str(e)}")
                    # ç¡®ä¿æ¯ä¸ªç»“æœéƒ½æœ‰å¿…è¦çš„å­—æ®µ
                    for res in raw_results:
                        if "final_score" not in res:
                            res["final_score"] = res["score"]
                        if "rerank_score" not in res:
                            res["rerank_score"] = res["score"]
                    reranked = sorted(raw_results, key=lambda x: x["score"], reverse=True)
                rerank_time = time.time() - rerank_start
            else:
                # å¦‚æœä¸ä½¿ç”¨é‡æ’åºï¼Œç›´æ¥ä½¿ç”¨æ··åˆæ£€ç´¢ç»“æœ
                logger.info("â© è·³è¿‡é‡æ’åºæ­¥éª¤ï¼Œç›´æ¥ä½¿ç”¨æ··åˆæ£€ç´¢ç»“æœ")
                # ç¡®ä¿resultsæœ‰æ‰€éœ€å­—æ®µ
                for res in raw_results:
                    res["final_score"] = res["score"]  # ä½¿ç”¨åŸå§‹åˆ†æ•°ä½œä¸ºæœ€ç»ˆåˆ†æ•°
                    res["rerank_score"] = res["score"]  # è®¾ç½®ç›¸åŒçš„rerank_scoreå€¼
                    res["original_score"] = res["score"]  # ä¿å­˜åŸå§‹åˆ†æ•°
                reranked = sorted(raw_results, key=lambda x: x["score"], reverse=True)

            # æ ¹æ®é˜ˆå€¼è¿‡æ»¤ç»“æœ
            filter_start = time.time()
            try:
                final_results = [
                    res for res in reranked
                    if res["final_score"] >= self.config.similarity_threshold
                    and len(res["doc"].page_content.strip()) >= 12  # æ·»åŠ é•¿åº¦æ£€æŸ¥
                ]
                final_results = sorted(
                    final_results,
                    key=lambda x: x["final_score"],
                    reverse=True
                )[:self.config.final_top_k]  # é™åˆ¶è¿”å›æ•°é‡
            except Exception as e:
                logger.error(f"ç»“æœè¿‡æ»¤å¤±è´¥ï¼Œä½¿ç”¨å‰Nä¸ªç»“æœ: {str(e)}")
                final_results = reranked[:min(len(reranked), self.config.final_top_k)]
            filter_time = time.time() - filter_start

            # è¾“å‡ºæœ€ç»ˆåˆ†æ•°ä¿¡æ¯å’Œæ—¶é—´ç»Ÿè®¡
            total_time = time.time() - start_time
            logger.info(f"ğŸ“Š æœ€ç»ˆæ–‡æ¡£æ•°ç›®:{len(final_results)}ç¯‡")
            
            # æ ¹æ®æ˜¯å¦ä½¿ç”¨é‡æ’åºè¾“å‡ºä¸åŒçš„æ—¥å¿—
            if use_rerank:
                logger.info(f"â±ï¸ æ£€ç´¢è€—æ—¶ç»Ÿè®¡: æ€»è®¡ {total_time:.3f}ç§’ | æ··åˆæ£€ç´¢: {hybrid_time:.3f}ç§’ | é‡æ’åº: {rerank_time:.3f}ç§’ | è¿‡æ»¤: {filter_time:.3f}ç§’")
            else:
                logger.info(f"â±ï¸ æ£€ç´¢è€—æ—¶ç»Ÿè®¡: æ€»è®¡ {total_time:.3f}ç§’ | æ··åˆæ£€ç´¢: {hybrid_time:.3f}ç§’ | è¿‡æ»¤: {filter_time:.3f}ç§’ (è·³è¿‡é‡æ’åº)")

            # æå–æ–‡æ¡£å’Œåˆ†æ•°ä¿¡æ¯
            docs = []
            score_info = []
            
            for res in final_results:
                try:
                    doc = res["doc"]
                    info = {
                        "source": res["source"],
                        "type": res.get("type", "unknown"),
                        "vector_score": res.get("score", 0),
                        "bm25_score": res.get("score", 0),
                        "rerank_score": res.get("rerank_score", res.get("score", 0)),
                        "final_score": res.get("final_score", res.get("score", 0))
                    }
                    docs.append(doc)
                    score_info.append(info)
                except Exception as e:
                    logger.warning(f"å¤„ç†å•ä¸ªç»“æœæ—¶å‡ºé”™ï¼Œå·²è·³è¿‡: {str(e)}")
                    continue

            return docs, score_info
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ£€ç´¢ä¸¥é‡å¤±è´¥: {str(e)}", exc_info=True)
            # ç´§æ€¥æƒ…å†µä¸‹è¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return [], []

    def _build_prompt(self, question: str, context: str) -> str:
        """æ„å»ºæç¤ºè¯æ¨¡æ¿"""
        # ç³»ç»Ÿè§’è‰²å®šä¹‰
        system_role = (
            "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„åŒ–å·¥å®‰å…¨é¢†åŸŸä¸“å®¶ï¼Œå…·æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†å’Œå®è·µç»éªŒã€‚"
            "ä½ éœ€è¦åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™ï¼Œç»™å‡ºå‡†ç¡®ã€ä¸“ä¸šä¸”æ˜“äºç†è§£çš„å›ç­”ã€‚"
        )
        
        # è¯¦ç»†å·¥ä½œæŒ‡å—
        instruction = (
            "å·¥ä½œæŒ‡å—ï¼š\n"
            "1. å¼•ç”¨çŸ¥è¯†ï¼šå›ç­”å¿…é¡»åŸºäºæ£€ç´¢åˆ°çš„å‚è€ƒèµ„æ–™å†…å®¹ï¼Œä¸è¦ç¼–é€ æˆ–è‡†æµ‹ä¿¡æ¯\n"
            "2. ä¸“ä¸šæ€§ï¼šä½¿ç”¨é€‚å½“çš„åŒ–å·¥å®‰å…¨æœ¯è¯­ï¼Œä¿æŒä¸“ä¸šæ€§\n"
            "3. å¯è¯»æ€§ï¼šå°†å¤æ‚æ¦‚å¿µè§£é‡Šå¾—æ¸…æ™°æ˜“æ‡‚ï¼Œé¿å…è¿‡åº¦ä½¿ç”¨ä¸“ä¸šæœ¯è¯­\n"
            "4. ç»“æ„æ€§ï¼šå›ç­”åº”æœ‰æ¸…æ™°çš„ç»“æ„ï¼Œå…ˆæ¦‚è¿°è¦ç‚¹ï¼Œå†è¯¦ç»†å±•å¼€\n"
            "5. å¼•ç”¨æ¥æºï¼šåœ¨å›ç­”ä¸­é€‚å½“å¼•ç”¨å‚è€ƒèµ„æ–™æ¥æºï¼Œå¯ä½¿ç”¨ã€Œæ ¹æ®XXæ–‡æ¡£ã€çš„å½¢å¼\n"
            "6. çŸ¥è¯†è¾¹ç•Œï¼šå¦‚å‚è€ƒèµ„æ–™ä¸åŒ…å«é—®é¢˜çš„ç­”æ¡ˆï¼Œå¦è¯šè¡¨æ˜ã€Œå‚è€ƒèµ„æ–™ä¸åŒ…å«æ­¤ä¿¡æ¯ã€\n"
        )
        
        # æ€è€ƒè¿‡ç¨‹æŒ‡å¯¼
        thinking_guide = (
            "æ€è€ƒè¿‡ç¨‹ï¼š\n"
            "1. ä»”ç»†é˜…è¯»å‚è€ƒèµ„æ–™ï¼Œè¯†åˆ«ä¸é—®é¢˜ç›¸å…³çš„å…³é”®ä¿¡æ¯\n"
            "2. åˆ†æé—®é¢˜éœ€æ±‚ï¼Œç¡®å®šå›ç­”æ¡†æ¶\n"
            "3. ç»„ç»‡ç›¸å…³ä¿¡æ¯ï¼Œå½¢æˆç³»ç»Ÿæ€§å›ç­”\n"
            "4. ç¡®ä¿å›ç­”å‡†ç¡®ã€å…¨é¢ä¸”ç¬¦åˆåŒ–å·¥å®‰å…¨é¢†åŸŸä¸“ä¸šæ ‡å‡†\n"
        )
        
        if context:
            return (
                "<|im_start|>system\n"
                f"{system_role}\n"
                f"{instruction}\n"
                f"{thinking_guide}\n"
                "å‚è€ƒèµ„æ–™ï¼š\n{context}\n"
                "è¯·æ ¹æ®ä»¥ä¸Šå‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœå‚è€ƒèµ„æ–™ä¸è¶³ä»¥å›ç­”ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚\n"
                "<|im_end|>\n"
                "<|im_start|>user\n"
                "{question}\n"
                "<|im_end|>\n"
                "<|im_start|>assistant\n"
            ).format(question=question, context=context[:self.config.max_context_length])
        else:
            return (
                "<|im_start|>system\n"
                f"{system_role}\n"
                f"{instruction}\n"
                "æ³¨æ„ï¼šæœªæ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„å‚è€ƒèµ„æ–™ï¼Œè¯·åŸºäºåŒ–å·¥å®‰å…¨é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†è°¨æ…å›ç­”ã€‚\n"
                "<|im_end|>\n"
                "<|im_start|>user\n"
                "{question}\n"
                "<|im_end|>\n"
                "<|im_start|>assistant\n"
            ).format(question=question)

    def _build_chat_prompt(self, current_question: str, chat_history: List[Dict], context: str = "") -> str:
        """æ„å»ºå¤šè½®å¯¹è¯çš„æç¤ºè¯æ¨¡æ¿
        
        :param current_question: å½“å‰ç”¨æˆ·é—®é¢˜
        :param chat_history: èŠå¤©å†å²è®°å½•åˆ—è¡¨ï¼ŒåŒ…å«message_typeå’Œcontent
        :param context: ç›¸å…³æ–‡æ¡£ä¸Šä¸‹æ–‡
        :return: å®Œæ•´çš„æç¤ºè¯
        """
        # ç³»ç»Ÿè§’è‰²å®šä¹‰
        system_role = (
            "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„åŒ–å·¥å®‰å…¨é¢†åŸŸä¸“å®¶ï¼Œå…·æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†å’Œå®è·µç»éªŒã€‚"
            "ä½ éœ€è¦åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™å’ŒèŠå¤©å†å²ï¼Œç»™å‡ºå‡†ç¡®ã€ä¸“ä¸šä¸”æ˜“äºç†è§£çš„å›ç­”ã€‚"
        )
        
        # è¯¦ç»†å·¥ä½œæŒ‡å—
        instruction = (
            "å·¥ä½œæŒ‡å—ï¼š\n"
            "1. å¼•ç”¨çŸ¥è¯†ï¼šå›ç­”å¿…é¡»åŸºäºæ£€ç´¢åˆ°çš„å‚è€ƒèµ„æ–™å†…å®¹ï¼Œä¸è¦ç¼–é€ æˆ–è‡†æµ‹ä¿¡æ¯\n"
            "2. ä¸“ä¸šæ€§ï¼šä½¿ç”¨é€‚å½“çš„åŒ–å·¥å®‰å…¨æœ¯è¯­ï¼Œä¿æŒä¸“ä¸šæ€§\n"
            "3. å¯è¯»æ€§ï¼šå°†å¤æ‚æ¦‚å¿µè§£é‡Šå¾—æ¸…æ™°æ˜“æ‡‚ï¼Œé¿å…è¿‡åº¦ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ä½¿å›ç­”éš¾ä»¥ç†è§£\n"
            "4. è¿è´¯æ€§ï¼šè€ƒè™‘å¯¹è¯å†å²ï¼Œä¿æŒå›ç­”çš„ä¸€è‡´æ€§\n"
            "5. å¼•ç”¨æ¥æºï¼šåœ¨å›ç­”ä¸­é€‚å½“å¼•ç”¨å‚è€ƒèµ„æ–™æ¥æºï¼Œå¯ä½¿ç”¨ã€Œæ ¹æ®XXæ–‡æ¡£ã€çš„å½¢å¼\n"
            "6. çŸ¥è¯†è¾¹ç•Œï¼šå¦‚å‚è€ƒèµ„æ–™ä¸åŒ…å«é—®é¢˜çš„ç­”æ¡ˆï¼Œå¦è¯šè¡¨æ˜ã€Œå‚è€ƒèµ„æ–™ä¸åŒ…å«æ­¤ä¿¡æ¯ã€\n"
        )
        
        # æ„å»ºç³»ç»Ÿæç¤ºéƒ¨åˆ†
        prompt = "<|im_start|>system\n" + system_role + "\n" + instruction + "\n"
        
        # æ·»åŠ å‚è€ƒèµ„æ–™ï¼ˆå¦‚æœæœ‰ï¼‰
        if context:
            prompt += "å‚è€ƒèµ„æ–™ï¼š\n" + context[:self.config.max_context_length] + "\n"
            prompt += "è¯·æ ¹æ®ä»¥ä¸Šå‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœå‚è€ƒèµ„æ–™ä¸è¶³ä»¥å›ç­”ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚\n"
        else:
            prompt += "æ³¨æ„ï¼šæœªæ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„å‚è€ƒèµ„æ–™ï¼Œè¯·åŸºäºåŒ–å·¥å®‰å…¨é¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†è°¨æ…å›ç­”ã€‚\n"
        
        prompt += "<|im_end|>\n"
        
        # æ·»åŠ èŠå¤©å†å²
        for message in chat_history:
            role = "user" if message["message_type"] == "user" else "assistant"
            content = message.get("content", "")
            if content:  # ç¡®ä¿æ¶ˆæ¯å†…å®¹ä¸ä¸ºç©º
                prompt += f"<|im_start|>{role}\n{content}\n<|im_end|>\n"
        
        # æ·»åŠ å½“å‰é—®é¢˜å’ŒåŠ©æ‰‹è§’è‰²
        prompt += f"<|im_start|>user\n{current_question}\n<|im_end|>\n"
        prompt += "<|im_start|>assistant\n"
        
        return prompt
        
    def _format_references(self, docs: List[Document], score_info: List[Dict]) -> List[Dict]:
        """æ ¼å¼åŒ–å‚è€ƒæ–‡æ¡£ä¿¡æ¯"""
        return [
            {
                "file": str(Path(info["source"]).name),  # æ–‡ä»¶å
                "content": doc.page_content,  # æˆªå–å‰500å­—ç¬¦
                "score": info["final_score"],  # ç»¼åˆè¯„åˆ†
                "type": info["type"],  # æ£€ç´¢ç±»å‹
                "full_path": info["source"]  # å®Œæ•´æ–‡ä»¶è·¯å¾„
            }
            for doc, info in zip(docs, score_info)
        ]


    def stream_query_with_history(self, session_id: str, current_question: str, 
                               chat_history: List[Dict] = None) -> Generator[str, None, None]:
        """å¸¦èŠå¤©å†å²çš„æµå¼RAGæŸ¥è¯¢
        
        :param session_id: ä¼šè¯ID
        :param current_question: å½“å‰ç”¨æˆ·é—®é¢˜
        :param chat_history: èŠå¤©å†å²åˆ—è¡¨
        :return: ç”Ÿæˆå™¨ï¼Œæµå¼è¾“å‡ºç»“æœ
        """
        logger.info(f"ğŸ”„ å¤šè½®å¯¹è¯å¤„ç† | ä¼šè¯ID: {session_id} | é—®é¢˜: {current_question[:50]}...")
        
        if not current_question.strip():
            yield json.dumps({
                "type": "error",
                "data": "è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜"
            }) + "\n"
            return
        
        # åˆå§‹åŒ–èŠå¤©å†å²
        if chat_history is None:
            chat_history = []
        
        try:
            # é˜¶æ®µ1ï¼šæ–‡æ¡£æ£€ç´¢
            try:
                docs, score_info = self._retrieve_documents(current_question)
                if not docs:
                    logger.warning(f"æŸ¥è¯¢ '{current_question[:50]}...' æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                    # å½“æ²¡æœ‰æ–‡æ¡£æ—¶ï¼Œä»ç„¶ä½¿ç”¨å†å²è®°å½•ï¼Œä½†æ— ä¸Šä¸‹æ–‡
                    context = ""
                else:
                    # æ ¼å¼åŒ–å‚è€ƒæ–‡æ¡£ä¿¡æ¯å¹¶å‘é€
                    references = self._format_references(docs, score_info)
                    yield json.dumps({
                        "type": "references",
                        "data": references
                    }) + "\n"
                    
                    # æ„å»ºä¸Šä¸‹æ–‡
                    context = "\n\n".join([
                        f"ã€å‚è€ƒæ–‡æ¡£{i + 1}ã€‘{doc.page_content}\n"
                        f"- æ¥æº: {Path(info['source']).name}\n"
                        f"- ç»¼åˆç½®ä¿¡åº¦: {info['final_score'] * 100:.1f}%"
                        for i, (doc, info) in enumerate(zip(docs, score_info))
                    ])
            except Exception as e:
                logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}", exc_info=True)
                # æ£€ç´¢å¤±è´¥æ—¶ä½¿ç”¨ç©ºä¸Šä¸‹æ–‡
                context = ""
                yield json.dumps({
                    "type": "error", 
                    "data": "æ–‡æ¡£æ£€ç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨èŠå¤©å†å²å›ç­”..."
                }) + "\n"
            
            # é˜¶æ®µ2ï¼šæ„å»ºå¤šè½®å¯¹è¯æç¤º
            prompt = self._build_chat_prompt(current_question, chat_history, context)
            
            # é˜¶æ®µ3ï¼šæµå¼ç”Ÿæˆ
            try:
                for chunk in self.llm.stream(prompt):
                    cleaned_chunk = chunk.replace("<|im_end|>", "")
                    if cleaned_chunk:
                        # å‘é€ç”Ÿæˆå†…å®¹
                        yield json.dumps({
                            "type": "content",
                            "data": cleaned_chunk
                        }) + "\n"
            except Exception as e:
                logger.error(f"æµå¼ç”Ÿæˆä¸­æ–­: {str(e)}")
                yield json.dumps({
                    "type": "error",
                    "data": "\nç”Ÿæˆè¿‡ç¨‹å‘ç”Ÿæ„å¤–ä¸­æ–­ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•"
                }) + "\n"
                
        except Exception as e:
            logger.exception(f"å¤šè½®å¯¹è¯å¤„ç†é”™è¯¯: {str(e)}")
            yield json.dumps({
                "type": "error",
                "data": " ç³»ç»Ÿå¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜"
            }) + "\n"
            
    def stream_query_model_with_history(self, session_id: str, current_question: str, 
                                 chat_history: List[Dict] = None) -> Generator[str, None, None]:
        """ç›´æ¥å¤§æ¨¡å‹çš„å¤šè½®å¯¹è¯æµå¼ç”Ÿæˆï¼ˆä¸ä½¿ç”¨çŸ¥è¯†åº“ï¼‰
        
        :param session_id: ä¼šè¯ID
        :param current_question: å½“å‰ç”¨æˆ·é—®é¢˜
        :param chat_history: èŠå¤©å†å²åˆ—è¡¨
        :return: ç”Ÿæˆå™¨ï¼Œæµå¼è¾“å‡ºç»“æœ
        """
        logger.info(f"ğŸ”„ ç›´æ¥å¤šè½®å¯¹è¯ | ä¼šè¯ID: {session_id} | é—®é¢˜: {current_question[:50]}...")
        
        if not current_question.strip():
            yield json.dumps({
                "type": "error",
                "data": "âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜"
            }) + "\n"
            return
        
        # åˆå§‹åŒ–èŠå¤©å†å²
        if chat_history is None:
            chat_history = []
        
        try:
            # æ„å»ºå¤šè½®å¯¹è¯æç¤ºï¼ˆæ— çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼‰
            prompt = self._build_chat_prompt(current_question, chat_history)
            
            # æµå¼ç”Ÿæˆ
            try:
                for chunk in self.llm.stream(prompt):
                    cleaned_chunk = chunk.replace("<|im_end|>", "")
                    if cleaned_chunk:
                        # å‘é€ç”Ÿæˆå†…å®¹
                        yield json.dumps({
                            "type": "content",
                            "data": cleaned_chunk
                        }) + "\n"
            except Exception as e:
                logger.error(f"ç›´æ¥å¤šè½®å¯¹è¯ç”Ÿæˆä¸­æ–­: {str(e)}")
                yield json.dumps({
                    "type": "error",
                    "data": "\nâš ï¸ ç”Ÿæˆè¿‡ç¨‹å‘ç”Ÿæ„å¤–ä¸­æ–­ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•"
                }) + "\n"
                
        except Exception as e:
            logger.exception(f"ç›´æ¥å¤šè½®å¯¹è¯å¤„ç†é”™è¯¯: {str(e)}")
            yield json.dumps({
                "type": "error",
                "data": "âš ï¸ ç³»ç»Ÿå¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜"
            }) + "\n"

    def answer_query(self, question: str) -> Tuple[str, List[Dict], Dict]:
        """éæµå¼RAGç”Ÿæˆï¼Œé€‚ç”¨äºè¯„ä¼°æ¨¡å—
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            Tuple(ç”Ÿæˆçš„å›ç­”, æ£€ç´¢çš„æ–‡æ¡£åˆ—è¡¨, å…ƒæ•°æ®)
        """
        logger.info(f"ğŸ” éæµå¼å¤„ç†æŸ¥è¯¢(ç”¨äºè¯„ä¼°): {question[:50]}...")
        
        try:
            # é˜¶æ®µ1ï¼šæ–‡æ¡£æ£€ç´¢
            try:
                docs, score_info = self._retrieve_documents(question)
                if not docs:
                    logger.warning(f"è¯„ä¼°æŸ¥è¯¢ '{question[:50]}...' æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                    return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚", [], {"status": "no_docs"}
            except Exception as e:
                logger.error(f"è¯„ä¼°æ¨¡å¼ä¸‹æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}", exc_info=True)
                return f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}", [], {"status": "retrieval_error", "error": str(e)}
            
            # æ ¼å¼åŒ–å‚è€ƒæ–‡æ¡£ä¿¡æ¯
            try:
                references = self._format_references(docs, score_info)
            except Exception as e:
                logger.error(f"æ ¼å¼åŒ–å‚è€ƒæ–‡æ¡£å¤±è´¥: {str(e)}")
                # åˆ›å»ºç®€åŒ–ç‰ˆå‚è€ƒä¿¡æ¯
                references = [{"file": f"æ–‡æ¡£{i+1}", "content": doc.page_content[:200] + "..."} 
                             for i, doc in enumerate(docs)]
            
            # é˜¶æ®µ2ï¼šæ„å»ºä¸Šä¸‹æ–‡
            try:
                context = "\n\n".join([
                    f"ã€å‚è€ƒæ–‡æ¡£{i + 1}ã€‘{doc.page_content}\n"
                    f"- æ¥æº: {Path(info['source']).name}\n"
                    f"- ç»¼åˆç½®ä¿¡åº¦: {info['final_score'] * 100:.1f}%"
                    for i, (doc, info) in enumerate(zip(docs, score_info))
                ])
            except Exception as e:
                logger.error(f"æ„å»ºä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")
                # å¦‚æœæ„å»ºä¸Šä¸‹æ–‡å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                context = "\n\n".join([f"ã€å‚è€ƒæ–‡æ¡£{i + 1}ã€‘{doc.page_content}" 
                                     for i, doc in enumerate(docs)])
            
            # é˜¶æ®µ3ï¼šæ„å»ºæç¤ºæ¨¡æ¿
            prompt = self._build_prompt(question, context)
            
            # é˜¶æ®µ4ï¼šä¸€æ¬¡æ€§ç”Ÿæˆï¼ˆéæµå¼ï¼‰
            try:
                answer = self.llm.invoke(prompt)
                cleaned_answer = answer.replace("<|im_end|>", "").strip()
                
                return cleaned_answer, references, {"status": "success"}
            except Exception as e:
                logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}")
                # å°è¯•ä½¿ç”¨ç®€åŒ–æç¤º
                try:
                    simple_prompt = (
                        "<|im_start|>system\n"
                        "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„åŒ–å·¥å®‰å…¨é¢†åŸŸä¸“å®¶ï¼Œè¯·å°½é‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n"
                        "<|im_end|>\n"
                        "<|im_start|>user\n"
                        f"{question}\n"
                        "<|im_end|>\n"
                        "<|im_start|>assistant\n"
                    )
                    fallback_answer = self.llm.invoke(simple_prompt)
                    cleaned_fallback = fallback_answer.replace("<|im_end|>", "").strip()
                    return cleaned_fallback, references, {"status": "partial_success", "error": str(e)}
                except:
                    return f"ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}", references, {"status": "generation_error", "error": str(e)}
            
        except Exception as e:
            logger.exception(f"éæµå¼å¤„ç†ä¸¥é‡é”™è¯¯: {str(e)}")
            return f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", [], {"status": "error", "error": str(e)}

